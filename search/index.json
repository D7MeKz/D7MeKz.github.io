[{"content":"TodoPoint 프로젝트에 대한 소개 Todopoint란 문자 그대로 할 일을 완수하면 포인트를 지급하는 애플리케이션 입니다. 계획을 제대로 완수하지 못한 사람들에게 강제성을 부여하고자 기획한 프로젝트입니다.\n프로젝트 기획의 창의성의 별개로 개인의 MSA에 대한 학습과 기술 역량을 향상하기 위해 시작한 프로젝트였습니다. 실제로 애플리케이션을 작동시키기 위해 쿠버네티스 홈 서버를 구축하기도 했습니다. 그러나 3개월이란 긴 시간 동안 예상치 못한 삽질로 제대로 마무리하지 못했습니다.\n지금부터 프로젝트 실패의 원인을 소개하겠습니다.\n잘못된 언어 선택 저는 웹 애플리케이션을 제작하면서 Go라는 언어와 Gin 웹애플리케이션 프레임워크를 사용했습니다. 그때 당시 Go 언어를 한번도 못해봤습니다. 단순하게 해보고 싶은 언어라서 선택했습니다. 이러한 선택으로 인해 예상치 못한 시간을 쓰게 되었습니다.\n새로운 기술을 사용할때는 고민해보자 앞서 말했듯이 그때 당시 Go언어는 처음이었습니다. 준비단계로 간단하게 Go 언어 문법만 익히고 바로 Gin을 쓰게 되었습니다.\nGo는 독특한 문법 체계를 가지고 있어 다른 언어에 비해 응용하기 어려웠습니다. 특히 구조를 짜는데 어려움을 겪었습니다. 객체지향 언어지만 흔히 알고 있는 클래스 기반 객체지향 패턴과는 다르다는 점이 어려웠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 //go:generate mockery --name Store --case underscore type Store interface { Create(ctx *gin.Context, info *data.UserInfo) error FindOne(ctx *gin.Context, uid int) (*data.Me, error) Update(ctx *gin.Context, uid int, me data.Me) error } type UserService struct { store Store } func NewUserService(store Store) *UserService { return \u0026amp;UserService{ store: store, } } func (s *UserService) Update(ctx *gin.Context, uid int, me data.Me) (*httputils.BaseResponse, *httputils.NetError) { err := s.store.Update(ctx, uid, me) if err != nil { return nil, httputils.NewNetError(codes.UpdateFailed, err) } return httputils.NewSuccessBaseResponse(nil), nil } 결국 깃허브에 베스트 케이스를 찾아다니면서 대략 한달 동안 코드를 2번 새로 만들었습니다. 주로 Error 구조체, 패키지 구조 등 부족하다고 생각하는 부분을 전부 다 수정했습니다.\n각각의 서비스를 만드는 것은 어렵지 않았습니다. 그러나 공통 모듈이라는 것을 만들려고 했을때 모든 것이 꼬이기 시작했습니다. 공통 모듈에는 필요한 인증 미들웨어나 전체적으로 통일된 형식의 구조체(http response값, 오류 메세지 타입 등)가 포함되어 있습니다. Go는 외부 라이브러리 저장소로 github를 주로 사용합니다. 공통 모듈을 github에 업로드하여 애플리케이션을 빌드할때마다 제작한 공통 모듈을 다운로드 받고 사용해야 할까요? 개발 초기 단계라 공통 모듈 수정이 잦습니다. 즉, 수정사항을 반영할때 비효율적입니다.\n이때 당시 \u0026ldquo;공통 모듈이 필요한가?\u0026ldquo;에 대해 많이 생각했던 것 같습니다. 저는 개인적으로 필요하다고 생각했습니다. 요구사항에 따라 다르겠지만 공통적인 부분이 많을수록 모듈화하여 관리하는 것이 효율적이라고 생각했기 때문이었습니다. 그대신어느정도 구조가 완성된 상태에서 사용해야 한다고 생각이 들었습니다.\n외부 다운로드 외에 다른 방법이 있었습니다. Go는 Workspace라는 개념이 있으며 go.work 파일을 통해 명시적으로 모듈을 포함시킬 수 있습니다.\n1 2 3 4 5 6 7 8 go 1.22.1 use ( . ./../../modules/v2/common ../../modules/v2/database/d7mysql ../../modules/v2/database/d7redis ) Workspace는 로컬 사용에 적합하다는 의견이 있습니다.1\n만든 Go 애플리케이션을 컨테이너로 실행시키기 위해서는 Dockerfile로 만들어야 합니다.\n우선 go 애플리케이션으로 빌드하기 위해서는 공통 모듈을 모두 복사하고 go work use 를 수행한 후 $GOPATH/src에 모듈을 저장하는 작업이 필요합니다. 그러나 개발 환경과 도커 내부 환경이 달라서 도커 내부에 go work use를 할 수 있도록 스크립트를 제작한 후 Dockerfile를 만들었습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # Stage 1: Build modules FROM golang:1.22 as modules-build COPY modules/v2/common /modules/common COPY modules/v2/database/d7redis /modules/database/d7redis COPY modules/v2/database/d7mysql /modules/database/d7mysql WORKDIR /app COPY scripts/init_workspace.sh /app/init_workspace.sh COPY auth-service/v2/workspace.packages.json /app/workspace.packages.json RUN apt-get update \u0026amp;\u0026amp; apt-get install -y jq RUN chmod +x init_workspace.sh \u0026amp;\u0026amp; ./init_workspace.sh RUN cat go.work COPY auth-service/v2 /app COPY auth-service/v2/go.mod /app/go.mod COPY auth-service/v2/go.sum /app/go.sum RUN go mod download RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /bin/main main.go # stage 2 FROM scratch COPY --from=modules-build /bin/main /bin/main EXPOSE 3001 CMD [\u0026#34;/bin/main\u0026#34;] 결국 해결하지 못했습니다. 경로 문제, 모듈이 제대로 설정되지 않는 문제 등 원인은 다양했습니다.\n해결책을 모색하다가 느낀건 \u0026ldquo;내가 Go를 적절하게 사용한 것이 맞나?\u0026rdquo; 였습니다. 그리고 거슬러 올라가 MSA를 채택한 것이 문제였다는 결론에 도달했습니다.\n마이크로서비스 하지 말았어야 했다. 결국 모든 원인은 아무 생각 없이 마이크로서비스를 도입했기 때문이라고 생각합니다. 그때 당시 MSA라고 불리는 용어가 트렌드처럼 떠올랐고, 규모가 있는 기업이 해당 능력을 요구하면서 능력이 필수라고 생각했습니다.\n왜 규모가 있는 기업이 MSA를 사용하는가? 에 대한 역질문을 생각하지 못한 것 같습니다.\n기업이 MSA를 채택한 이유는 서비스 간 결합도를 낮춰 요구사항에 대한 빠른 대처하기 위해서 라고 생각합니다. 제 프로젝트는 MSA로 할만큼 큰 서비스가 아니였고 이를 구현할 능력을 가지고 있지 않았습니다. 즉 오버 엔지니어링을 한 것이었습니다.\n실패 이후의 새로운 시작 이 프로젝트를 수행하면서 많은 시간을 사용했습니다. 노력 끝에 완성된 것이 하나 없으니 한편으로는 아쉽다는 생각이 들었습니다.\n하지만 이 프로젝트가 저에게는 새로운 시작을 알렸던 것 같습니다.\n\u0026ldquo;기초부터 공부할 예정이라면 기본적인 구성부터 공부해보자.\u0026rdquo; 라는 생각이 들었습니다. 이후로 저는 기초부터 공부할 수 있도록 작은 프로젝트를 기획하게 되었습니다.\nhttps://www.reddit.com/r/golang/comments/19517hq/what_are_you_using_go_workspaces_for_if_at_all/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-10-09T00:00:00Z","permalink":"https://s0okju.github.io/p/todopoint-final-review/","title":"왜 TodoPoint 프로젝트를 실패했는가?"},{"content":"DevStack의 한계 DevStack으로 Openstack을 배포하면서 큰 문제점을 느끼게 되었습니다.\nNova Instance에 직접 접근하는 것이 어렵다. 리소스를 추가 및 삭제하는 것이 어렵다. 위의 문제점은 네트워크로 인해 발생한 것입니다. 예로 제가 Nova Instance를 생성해 Floating ip를 할당한다고 가정해 봅시다. External Network의 IP 범위가 외부와 연결이 불가능해 직접적으로 접근할 수 없게 됩니다. 그럼 ssh로 서버에 직접 접속해서 관리하는 방안도 있습니다. 그럼 Openstack을 왜 쓴거지? 키는 어떻게 관리하지? 등 다양한 꼬리 질문이 따라오게 됩니다.\nKolla Ansible 방안을 모색하는 중 Openstack은 DevStack 이외에도 다양한 배포 방안이 있다는 것을 알게 되었습니다.\nKolla Ansible이란 Ansible 플레이북을 통해 OpenStack의 모든 서비스를 컨테이너로 배포하고 유지 보수하며 운영할 수 있도록 도와주는 프로젝트이다. 이를 통해 복잡한 설치 과정과 수동 설정을 최소화하고, 재현 가능한 환경을 제공하여 효율적이며 안정적인 클라우드 인프라 운영을 가능하게 한다.1\n즉 우리가 설정한 시스템 서비스를 컨테이너 형태로 구축되는 것입니다.\n설치 설치는 파란돌님의 블로그 - Kolla-ansible로 Openstack All-in-one 설치하기(Installing Openstack All-in-one with Kolla-ansible)를 적극 참고했습니다.\n설치 목록 Kolla-ansible의 장점은 원하는 리소스 선택과 설정이 쉽다는 것입니다. 그래서 저는 기본적인 리소스를 선택하여 설치했습니다.\n설치할 리소스\nKeystone Glance Nova Neutron Cinder Swift Horizon all-in-one or node? 오픈스택은 크게 3가지 노드가 있습니다.\nController node : 전체 오픈스택 서비스를 관리하기 위해 사용됩니다. 컨트롤러 관리 및 노드 간 연결을 의해서는 최소한 2개 이상의 인터넷 인터페이스가 필요합니다. Compute node : Nova 기반의 인스턴스를 작동하기 위해 사용되는 하이퍼바이저를 실행하는 노드입니다. Network node : 다양한 네트워크 서비스 에이전트를 실행하며, 이를 가상 네트워크에 인스턴스를 연결합니다. 여러 가이드를 보면 노드들은 물리적으로 분리되어 있습니다. 서버 내 가상머신을 구축하여 구현하는 방법이 있습니다. 그러나 복잡한 관계로 서비스 기능을 하나의 호스트에 설치할 수 있는 all-in-one2을 선택하게 되었습니다.\nOpenstack 네트워크 가장 어렵고 앞으로 계속 공부해야 하는 분야인 것 같습니다. 완전히 이해한 것은 아니지만 알고 있는 그대로 작성하겠습니다.\n오픈 스택에서 네트워크는 Management, Tunnel, External 네트워크가 있습니다.\nManagement Network : 관리용 네트워크로 각 컴포넌트와 관련된 API를 호출하는데 사용됩니다. Tunnel Network : vm instance 간 네트워크를 구축하는데 사용됩니다. External Network : vm instance가 인터넷과 통신하기 위한 네트워크입니다. 네트워크 서비스는 크게 두 가지 옵션이 있습니다.\nProvider Network : 가상 네트워크를 물리적 네트워크로 연결합니다. 즉 물리적인 네트워크가 vm 인스턴스가 활용하는 네트워크가 됩니다. Self-Service Network : 오픈스택을 사용하는 사용자가 직접 자신만의 네트워크를 구축할 수 있는 네트워크 입니다. Provider 네트워크는 부하분산 서비스 혹은 방화벽 서비스 등 고급 기능을 지원하지 않습니다.\n제가 사용하는 옵션은 비교적 간단한 Provider Network 입니다.\n테스트용 Openstack 서버의 네트워크 접근 문제 kolla-ansible에서 제공해주는 globals.yml 를 확인해보겠습니다. 환경 파일을 보면 Internal, External Network로 구분되어 있습니다.\nnetwork_interface는 Internal network에 해당되는 인터페이스를 지정하는 것으로 저는 외부 인터넷과 연결되지 않는 enp2s0 인터페이스를 사용했습니다. 반면 neutron_external_interface는 provider로 제공할 인터페이스로 인터넷과 연결할 수 있는 enp3s0 인터페이스를 사용했습니다.\n1 2 3 4 5 6 7 8 9 ############################## # Neutron - Networking Options ############################## # ... # followed for other types of interfaces. network_interface: \u0026#34;enp2s0\u0026#34; --- # ... neutron_external_interface: \u0026#34;enp3s0\u0026#34; 그림을 그려보면 아래와 같습니다.\n(아래의 그림은 172.17.0.250/24이 아니라 172.16.0.250/24 입니다. )\n하지만 이번 오픈스택 서버는 어디까지나 개인으로만 사용되는 공간입니다. 즉 클라우드 서비스를 쓰는 것도 저 혼자이고, 관리하는 것도 저 혼자인 것입니다. 관리를 위한 네트워크 접속(Internal Network)도, 오픈스택 리소스 접근할 수 있는 네트워크 접속(External Network)도 가능해야 합니다.\n그러나 저는 클라우드 리소스의 API를 접근할 수 없습니다. 왜냐하면 서로 다른 네트워크 대역을 가지고 있기 때문입니다.\n이러한 문제를 해결하기 위해서는 라우팅 테이블을 설정해야 할 것입니다. 라우터에 직접 설정하는 방법과 운영체제에서 처리하는 방법이 있는데, 물리적으로 라우터가 하나만 존재하기 때문에 운영체제에서 처리해야 합니다.\n저는 ip forward를 사용했습니다.\nIP-Forward란 커널 기반 라우팅 포워딩으로 하나의 인터페이스로 들어온 패킷을 다른 서브넷을 가진 네트워크 인터페이스로 패킷을 포워딩시키는 것이다.\n1 2 3 # Kernel parameter update $ sudo sysctl -w net.ipv4.conf.all.forwarding=1 $ sudo sysctl net.ipv4.conf.all.forwarding net.ipv4.conf.all.forwarding = 1 br-ex 송신되어 enp2s0 인터페이스에 수신되는 것을 허락한다는 의미입니다.\nbr-ex은 OpenVSwitch(OVS) 적용시 물리 네트워크 인터페이스와 직접 연결되는 가상 스위치입니다. 외부 네트워크에서 가상 머신을 접근할때 사용됩니다.\n1 2 sudo iptables -I FORWARD -i br-ex -o enp2s0 -j ACCEPT sudo iptables -nL FORWARD 클라이언트도 설정이 필요합니다.\n192.168.50.27를 통해 172.16.0.0/24(enp2s0)에 접근한다는 routing rule를 추가합니다.\n1 sudo route -n add 172.16.0.0/24 192.168.50.27 여기서 왜 192.168.50.27를 통해 172.16.0.0/24 네트워크에 접근한다고 설정한 것일까요?\n오픈 스택 서버에서 ip_forward를 시켰습니다. 즉 서버가 라우터의 역할을 하는 것입니다. 그래서 클라이언트에서 라우팅 룰을 설정할 때에는 172.16.0.250/24에 포워딩을 시킨 서버의 ip 주소로 설정해야 하는 것입니다.\n정리 테스트를 수행하기 위해 Openstack 서버를 구축하게 되었습니다. 구축 시 요구사항은 인스턴스에 접근할 수 있으면서 리소스 API에 접근 가능해야 한다는 것이었습니다. 서로 다른 네트워크 대역을 가져 리소스 API에 접근하지 못했으나 서버에 ip forward를 수행하여 문제를 해결했습니다.\n문제를 해결해 보니 Neutron에 대해 모르고 있다는 사실을 알게 되었습니다. 다음 포스팅은 Neutron 톱아보기로 돌아오겠습니다.\nReference https://velog.io/@lijahong/0%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-Linux-%EA%B3%B5%EB%B6%80-%EB%B0%A9%ED%99%94%EB%B2%BD-%EC%BB%B4%ED%93%A8%ED%84%B0 https://velog.io/@larshavin/Kolla-Ansible%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-OpenStack-%EC%84%A4%EC%B9%98-1 https://blog.naver.com/love_tolty/220237750951 https://tech.osci.kr/openstack_cinder/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://blog.naver.com/love_tolty/220237750951\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-10-01T00:00:00Z","permalink":"https://s0okju.github.io/p/server-setup-4/","title":"서버 구축기 - 4. Kolla-ansible 설치 시 마주한 네트워크 문제"},{"content":"JVM의 메모리 사용 형태 JVM에는 3가지 메모리 사용 형태가 있습니다.\nused: JVM이 사용 중인 메모리 양 committed: JVM이 시스템에게 요청한 메모리 양 max : 실행 중인 환경에서 최대로 할당할 수 있는 메모리 양 메모리 용량은 used \u0026lt;= committed \u0026lt;= max를 따르는데, used 메모리 양이 committed, max 메모리보다 크게 된다면 OOM(Out Of Memory)이 발생1하게 됩니다.\n그라파나를 확인해보면 3가지의 메모리 형태를 알 수 있습니다.\nCommitted Memory의 역할 YGwan님의 블로그: JVM Memory의 3가지 메트릭 지표 - Committed Memory란?를 읽어보는 것을 추천드립니다.\n메모리를 할당하고 해제하는데 필요한 오버헤드를 줄일 수 있다는 특징을 가지고 있습니다.\nJVM이 시스템을 통해 미리 메모리를 할당하여 사용 시 가지고 있는 여유 메모리 공간을 활용합니다. 이를 통해 메모리를 즉시 사용할 수 있으며 메모리를 할당하는데 발생하는 오버헤드를 줄일 수 있게 됩니다.\nAdvanced echo server jvm 메트릭 특징 이전 글에서 스프링부트 컨테이너의 메모리가 지속적으로 증가한다고 언급했습니다. 그래서 저는 jvm에 문제가 있을 수 있겠다는 생각이 들어 관련 지표를 확인해보겠습니다.\n처음에 눈여겨 봤던 것은 JVM의 committed 메모리 양이 점진적으로 증가하고 있다는 점이었습니다.\n😂 사진에서는 잘 보이지 않지만 자세히 보면 1MB씩 상승하고 있습니다.\n자세히 알아보기 전에 heap과 non-heap 메모리에 대해 알아봅시다.\nJava Heap : Instance와 객체가 저장되는 공간으로 모든 Thread에 의해 공유되는 영역2 Native Area(non-heap) : OS레벨에서 관리하는 영역으로 JVM 크기에 강제되지 않기 때문에 프로세스가 이용할 수 있는 메모리 자원을 최대한 활용할 수 있습니다. 다시 돌아와 제가 운영하고 있는 웹 애플리케이션을 확인해 볼까요.\n정말 간단한 EchoController와 프로메테우스 metric 수집을 위한 MetricController가 있습니다.\n1 2 3 4 5 6 7 8 @RestController public class EchoController { @GetMapping(\u0026#34;/echo\u0026#34;) public String echo() { return \u0026#34;Hello, World!\u0026#34;; } } 웹 애플리케이션이 비교적 단순하기 때문에 Heap 영역의 문제는 아니라고 생각했습니다.\n그래도 JVM Heap를 확인해 보겠습니다. 힙 메모리의 사용량은 매우 적게 사용되었고, 안정적으로 유지되었습니다.\nnon-heap 영역을 봐봅시다.\n변화폭이 적어서 차이를 모르겠지만 메모리가 사용량이 조금씩 증가하고 있습니다.\n우선 프로파일링은 무엇을 의미할까요. hudi.blog: 스프링 애플리케이션 배포 직후 발생하는 Latency의 원인과 이를 해결하기 위한 JVM Warm-up를 참고하겠습니다.\nJIT 컴파일러는 애플리케이션에서 자주 실행된다고 판단되는 특정 부분만을 기계어로 컴파일한다. 이 부분을 핫스팟(Hotspot) 이라고 부른다. JIT 컴파일러는 실행중인 애플리케이션의 동작을 분석하고 코드 실행 횟수, 루프 반복 횟수, 메소드 호출 등의 정보를 측정하고 기록한다. 이를 프로파일링이라고 한다.\nJIT 컴파일러는 프로파일링 결과를 토대로 핫스팟을 식별한다. 핫스팟이 식별되었다면, JIT 컴파일러는 메소드 단위로 바이트 코드를 기계어로 번역한다. JIT 컴파일러는 이렇게 번역된 기계어를 코드 캐시(Code Cache) 라는 캐시공간에 저장한다.\n출처 - https://hudi.blog/jvm-warm-up/\nJIT는 전체 코드를 컴파일 하는 것이 아닌 자주 실행되는 부분에만 컴파일하며 이에 대한 데이터를 Code Cache에 저장하게 됩니다.\nNon Heap과 관련된 지표를 확인해봅시다.\n아래의 세 가지 지표가 미세하게 증가하고 있습니다.\nMetaspace : JVM이 로드한 메타데이터를 저장 CodeHeap : JIT 컴파일러 관련 지표 profiled nmethods non-profiled nmethods 하지만 장기적인 관점에서 바라봤을때 메모리 낭비일 수 있겠다는 생각이 들었습니다.\n특히 Metaspace는 JVM 크기에 강제되지 않기 때문에 프로세스가 사용할 수 있는 메모리 자원을 최대한 활용할 수 있게 됩니다. 그러므로 Metaspace 메모리를 제한하여 운영할 필요가 있습니다.\n이러한 성능 제한은 Metaspace에만 적용되는 것이 아니라 다른 영역에서의 메모리 사이즈 제한이 필요합니다.\n안전하게 애플리케이션을 운영하기 위해서는 각 메모리 영역에 맞는 요구사항을 고려하여 결정하는 것이 바람직합니다.\nJava 어플리케이션은 크게 위의 Heap과 Off-Heap 두 공간을 활용하여 동작하는데, 따라서 어플리케이션을 배포할 때 메모리 몇 GB를 할당해야 하는지 결정하기 위해서는 단순히 Xmx(Heap 메모리 최대치를 결정하는 Java 옵션) 값만 생각하면 OOME에 빠지기 쉽다. 실제로는 Xmx에 MaxMetaspace값을 더하고, 추가로 프로그램에서 NIO를 사용해 Native Memory를 직접 할당받는 로직을 고려해서 Heap + Native Memory 사용총량으로 할당을 해야 비교적 정확하다. 특히 컨테이너의 경우 계산을 좀 더 정확하게 해야 시스템에서 OOM killed되는 상황을 면할 수 있다.\n출처 - https://www.samsungsds.com/kr/insights/1232761_4627.html\n정리 실제로 배포해보면서 컨테이너의 지속적인 메모리 사용량이 증가하는 것에 의문점을 가지게 되었습니다. 자바 컨테이너의 메모리 사용은 JVM이 컨트롤할 수 없는 부분이 많아 추적하기 어렵습니다3.\n그래서 자세히 확인할 수 있는 JVM 위주로 확인하게 되었습니다. non-heap 영역이 미세하게 상승하고 있었고, 이는 정상적인 지표라고 생각했습니다. 그러나 장기적으로는 메모리 낭비가 될 수 있으므로 메모리 사이즈를 제한하여 운영해야 합니다.\nReference https://swmobenz.tistory.com/37 https://hudi.blog/jvm-warm-up/ https://velog.io/@dongvelop/Spring-Boot-%EC%84%9C%EB%B2%84-%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81-%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C%EB%B3%84-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EC%9C%A0%EC%9D%98%EC%82%AC%ED%95%AD#codeheap-non-profiled-nmethods-non-heap https://www.samsungsds.com/kr/insights/1232761_4627.html https://obv-cloud.com/41 https://swmobenz.tistory.com/37\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://12bme.tistory.com/382\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/questions/53451103/java-using-much-more-memory-than-heap-size-or-size-correctly-docker-memory-limi\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-09-29T00:00:00Z","permalink":"https://s0okju.github.io/p/opsathlan-2-2/","title":"OpsAthlan 2 - 2. Advanced echo server 메트릭 분석"},{"content":"시나리오 이전글 을 보니 스프링 애플리케이션 컨테이너에 문제가 생긴 것 같다. spring boot 애플리케이션에 대한 지표를 수집할 수 있도록 환경을 구축해보자.\n환경 Ubuntu 22.04 Openstack 2024.1 전체 구상도 아래와 같이 구상됩니다. cAdvisor를 설정한 이유는 컨테이너 관련 메트릭을 수집하기 위해 설정했습니다. docker-compose를 통해 컨테이너를 배포했습니다.\n쿠버네티스는 컨테이너들의 리소스 지표를 제공하기 위해 cAdvisor를 기본적으로 제공합니다.1\n절차 Terraform, Ansible를 활용한 환경 구성 프로메테우스를 활용한 지표 수집 그라파나 시각화 환경 구성 클라우드 리소스의 구성 자체는 이전과 변화하지 않았습니다.\ndocker compose 다만 환경 구성은 변했습니다. 개별 컨테이너로 배포하는 것이 아닌 docker-compose를 통해 관련 컨테이너를 배포했습니다. 그 이유는 컨테이너의 개수가 많아짐에 따라 컨테이너를 큰 묶음으로 관리하기 위해서입니다.\n총 4개의 컨테이너를 배포하게 됩니다\nPrometheus Grafana Springboot application cAdvisor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # docker-compose.yml version: \u0026#34;3.7\u0026#34; services: # Prometheus service prometheus: image: prom/prometheus:latest container_name: prometheus volumes: - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml ports: - \u0026#34;9090:9090\u0026#34; # Expose Prometheus web UI networks: - monitoring # Grafana service grafana: image: grafana/grafana:latest container_name: grafana ports: - \u0026#34;3000:3000\u0026#34; # Expose Grafana web UI networks: - monitoring depends_on: - prometheus # Spring Boot app service advanced-echo: image: opsathlan-advanced-echo:latest container_name: opsathlan-advanced-echo ports: - \u0026#34;8080:8080\u0026#34; networks: - monitoring # cadvsior config cadvisor: container_name: cadvisor image: gcr.io/cadvisor/cadvisor:latest ports: - \u0026#34;8081:8080\u0026#34; volumes: - \u0026#34;/:/rootfs:ro\u0026#34; - \u0026#34;/var/run:/var/run:ro\u0026#34; - \u0026#34;/sys:/sys:ro\u0026#34; - \u0026#34;/var/lib/docker/:/var/lib/docker:ro\u0026#34; # docker container log networks: - monitoring networks: monitoring: driver: bridge 프로메테우스 프로메테우스가 수집한 메트릭 지표는 springboot container 관련 지표와 컨테이너 관련 지표입니다. 그래서 scrape_configs target에 관련 컨테이너 URL를 기입하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 # prometheus.yml global: scrape_interval: 10s # How often to scrape targets scrape_configs: - job_name: \u0026#34;opsathlan-advanced-echo\u0026#34; static_configs: - targets: [\u0026#34;opsathlan-advanced-echo:8080\u0026#34;, \u0026#34;cadvisor:8080\u0026#34;] labels: group: \u0026#34;test\u0026#34; docker compose를 실행하는 Ansible playbook nova에서 docker-compose를 실행시키기 위해서는 관련 바이너리를 설치하고, 필요한 환경 파일을 인스턴스에 복사하는 작업이 필요합니다. docker-compose 라는 ansible role를 만들어 스프링 컨테이너 이미지를 빌드하고, docker compose를 실행시키는 테스크를 추가했습니다.\n개인적으로 작업한 도커 설치 ansible-role 를 확인해보면 docker 설치할때 docker-compose plugin를 설치하게 됩니다. 그래서 별도의 설치 없이 docker compose 명령어 만으로도 docker-compose 파일을 실행시킬 수 있게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 - name: Update apt cache apt: update_cache: yes - name: Install Docker Engine, CLI, containerd, and plugins apt: name: - docker-ce - docker-ce-cli - containerd.io - docker-buildx-plugin - docker-compose-plugin state: present 프로메테우스를 실행시키기 위해서는 config 파일을 nova 인스턴스에 직접 저장해야 하는 작업이 필요합니다. docker-compose.yml 를 다시 확인해보면 local volume이 ./prometheus/prometheus.yml 로 되어 있습니다. 파일 경로는 nova 인스턴스의 파일 경로로 해당 파일 경로에 맞게 설정 파일(prometheus.yml)를 저장해야 합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # docker-compose.yml services: # Prometheus service prometheus: image: prom/prometheus:latest container_name: prometheus volumes: - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml ports: - \u0026#34;9090:9090\u0026#34; # Expose Prometheus web UI networks: - monitoring prometheus의 폴더를 만든 후에 복사하는 작업을 하면 파일이 저장됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 - name: Create Prometheus configuration directory file: path: \u0026#34;/home/ubuntu/prometheus\u0026#34; state: directory owner: \u0026#34;ubuntu\u0026#34; group: \u0026#34;ubuntu\u0026#34; mode: \u0026#34;0755\u0026#34; - name: Copy Prometheus config file copy: src: ./prometheus.yml dest: /home/ubuntu/prometheus/prometheus.yml owner: \u0026#34;ubuntu\u0026#34; group: \u0026#34;ubuntu\u0026#34; mode: \u0026#34;0644\u0026#34; docker compose 명령어를 수행하면 docker-compose 파일에 정의된 컨테이너들을 실행시킬 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 - name: Copy docker-compose.yml to the target machine copy: src: ./docker-compose.yml dest: /home/ubuntu/docker-compose.yml # Destination on the target machine owner: \u0026#34;ubuntu\u0026#34; group: \u0026#34;ubuntu\u0026#34; mode: \u0026#34;0644\u0026#34; - name: Run docker-compose up to start containers command: docker compose up -d args: chdir: /home/ubuntu/ become: true 프로메테우스를 활용한 지표 수집 메트릭 수집, 시각화, 알림, 서비스 디스커버리 기능을 모두 제공하는 오픈 소스 모니터링 시스템입니다. Pull 방식으로 메트릭을 수집하고 시계열 데이터베이스에 저장하는 특징을 가지고 있습니다.\npull 방식은 프로메테우스가 필요한 메트릭 지표를 대상 서버로부터 가져온다는 것과 같은 말입니다. 그래서 대상이 되는 서버는 프로메테우스가 메트릭을 받아갈 수 있도록 endpoint를 열어둬야 합니다.\n보통의 방식이라면 spring boot actuator를 활용하여 /actuator/metrics 엔드포인트를 여는 방법을 사용할 수 있습니다.\n1 2 3 4 5 # application.properties management.endpoints.web.exposure.include=prometheus,metrics management.endpoint.prometheus.enabled=true management.prometheus.metrics.export.enabled=true 위와 같은 방식을 사용하게 되면 EOF 오류가 발생해 프로메테우스가 정상적으로 지표를 가져올 수 없었습니다. 그래서 header를 TEXT_PLAIN으로 지정해주는 controller를 생성했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import io.micrometer.prometheusmetrics.PrometheusMeterRegistry; @RestController @RequiredArgsConstructor public class MetricsController { private final PrometheusMeterRegistry prometheusMeterRegistry; @GetMapping(\u0026#34;/metrics\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; metrics() { String scrape = prometheusMeterRegistry.scrape(); HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.TEXT_PLAIN); return new ResponseEntity\u0026lt;\u0026gt;(scrape, headers, HttpStatus.OK); } } springboot actuator 스프링부트 애플리케이션 모니터링과 관리에 필요한 기능을 제공합니다.\n9090 포트에 접속해 프로메테우스 web ui를 보면 스프링부트 애플리케이션과 cAdvisor 지표가 정상적으로 통신됨을 알 수 있습니다.\n그라파나를 통한 시각화 cAdvisor 지표 시각화 그라파나에서 container의 메모리 사용량을 확인해봅니다. 메모리 단위로 확인하기 위해 1024를 두번 나눴습니다.\n관련 지표는 cadvisor Github를 참고하길 바랍니다.\n1 container_memory_usage_bytes{name=\u0026#34;instance_name\u0026#34;}/1024/1024 아래의 그래프를 보면 계속 상승하고 있는 그래프를 확인할 수 있습니다.\n스프링 애플리케이션 관련 지표 시각화 저는 GrafanaLabs에서 제공해주는 JVM 대시보드를 사용했습니다. dashboard ip를 설정하면 다양한 그래프를 확인할 수 있습니다.\n아래의 사진은 일부만 나타낸 것이며 이것 이외에도 다양한 지표가 있습니다.\n다음글 지금까지 모니터링할 수 있는 환경을 구축했습니다. 다음편에는 시각화한 표를 분석해보는 시간을 가지도록 하겠습니다.\nhttps://themapisto.tistory.com/44\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-09-26T00:00:00Z","permalink":"https://s0okju.github.io/p/opsathlan-2-1/","title":"OpsAthlan 2 - 1. Advanced echo server 구축"},{"content":"시나리오 Hello World를 반환해주는 springboot 도커 서비스가 있다. 배포해보자!\n환경 ubuntu 22.04 openstack 2024.01 순서 Terraform을 활용한 인프라 구성 Ansible를 활용한 환경 구성 및 빌드 cAdvisor를 활용한 모니터링 cAdvisor를 선택한 이유 cAdvsior는 컨테이너의 Metrix를 모니터링할 수 있는 도구입니다. 기본적인 컨테이너 메트릭 정보를 우선 확인하기 위해 선택하게 되었습니다.\n전체 구성도 Terraform을 활용한 리소스 배포 배포해야 할 것은 아래와 같습니다.\nNova Instance Network Subnet Router Floating ip Security Security Group ssh, spring web port(8080), cAdvisor(8081)를 위한 rule 생성 ssh 통신을 위한 keypair Terraform code terraform은 절차 지향적이기 때문에 리소스 작성 순서가 중요합니다.그래서 간단한 nova 인스턴스 배포의 경우 아래와 같은 순서를 따랐습니다.\n네트워크 인스턴스 생성 floating ip 할당 전체 코드는 깃허브를 참고하길 바랍니다.\n네트워크를 생성한 이후 subnet를 생성한다. 외부와 통신하기 위해서는 external_network와 서브넷과의 연결이 필요합니다. 그러므로 router를 생성한 후 router_interface를 통해 external_network와 서브넷이 연결하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Network 생성 resource \u0026#34;openstack_networking_network_v2\u0026#34; \u0026#34;opsathlan_network\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-network\u0026#34; admin_state_up = true port_security_enabled = var.port_security_enabled } # Subnet 생성 resource \u0026#34;openstack_networking_subnet_v2\u0026#34; \u0026#34;opsathlan_subnet\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-subnet\u0026#34; network_id = openstack_networking_network_v2.opsathlan_network.id cidr = var.subnet_cidr ip_version = 4 dns_nameservers = [\u0026#34;8.8.8.8\u0026#34;, \u0026#34;8.8.4.4\u0026#34;] } # Router 생성 resource \u0026#34;openstack_networking_router_v2\u0026#34; \u0026#34;opsathlan_router\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-router\u0026#34; admin_state_up = true external_network_id = data.openstack_networking_network_v2.ext_network.id } # Router Interface 생성 resource \u0026#34;openstack_networking_router_interface_v2\u0026#34; \u0026#34;router_interface\u0026#34; { router_id = openstack_networking_router_v2.opsathlan_router.id subnet_id = openstack_networking_subnet_v2.opsathlan_subnet.id } 인스턴스 생성 인스턴스에 접근하는데 사용되는 keypair를 생성합니다.\n1 2 3 4 5 # Keypair 생성 resource \u0026#34;openstack_compute_keypair_v2\u0026#34; \u0026#34;opsathlan_keypair\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-keypair\u0026#34; public_key = file(var.pubkey_file_path) } Nova 인스턴스의 접근 제어를 관리하는 Security group를 설정합니다. security group rule은 variables.tf 에서 리스트로 받은 후에 적용할 수 있도록 구성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Security Group 생성 resource \u0026#34;openstack_networking_secgroup_v2\u0026#34; \u0026#34;opsathlan_secgroup\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-secgroup\u0026#34; description = \u0026#34;OpsAthaln Security Group\u0026#34; delete_default_rules = false } # Security Group Rule 생성 resource \u0026#34;openstack_networking_secgroup_rule_v2\u0026#34; \u0026#34;secgroup_rules\u0026#34; { count = length(var.allowed_ports) direction = \u0026#34;ingress\u0026#34; ethertype = \u0026#34;IPv4\u0026#34; protocol = lookup(var.allowed_ports[count.index], \u0026#34;protocol\u0026#34;, \u0026#34;tcp\u0026#34;) port_range_min = lookup(var.allowed_ports[count.index], \u0026#34;port_range_min\u0026#34;) port_range_max = lookup(var.allowed_ports[count.index], \u0026#34;port_range_max\u0026#34;) remote_ip_prefix = lookup(var.allowed_ports[count.index], \u0026#34;remote_ip_prefix\u0026#34;, \u0026#34;0.0.0.0/0\u0026#34;) security_group_id = openstack_networking_secgroup_v2.opsathlan_secgroup.id } variables.tf 에서는 외부에서 접근할 수 있는 포트를 설정합니다.\nssh spring boot application(8080) cAdvisor(8081) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // variables.tf variable \u0026#34;allowed_ports\u0026#34; { description = \u0026#34;List of maps defining allowed ports for the security group. Each map should include protocol, port_range_min, port_range_max, and optionally remote_ip_prefix.\u0026#34; type = list(object({ protocol = string port_range_min = number port_range_max = number remote_ip_prefix = optional(string, \u0026#34;0.0.0.0/0\u0026#34;) })) default = [ { protocol = \u0026#34;tcp\u0026#34; port_range_min = 22 port_range_max = 22 }, { protocol = \u0026#34;tcp\u0026#34; port_range_min = 8080 port_range_max = 8080 }, { protocol = \u0026#34;tcp\u0026#34; port_range_min = 8081 port_range_max = 8081 } ] } Floating ip 할당 및 설정 floating ip를 생성한 후에 nova instance에 붙힙니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 5. Floating IP 생성 및 연결 resource \u0026#34;openstack_networking_floatingip_v2\u0026#34; \u0026#34;floating_ip\u0026#34; { pool = data.openstack_networking_network_v2.ext_network.name } # 4. Nova Instance 생성 resource \u0026#34;openstack_compute_instance_v2\u0026#34; \u0026#34;nova_instance\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-instance\u0026#34; image_name = var.image_name flavor_name = var.flavor_name key_pair = openstack_compute_keypair_v2.opsathlan_keypair.name security_groups = [openstack_networking_secgroup_v2.opsathlan_secgroup.name] network { uuid = openstack_networking_network_v2.opsathlan_network.id } depends_on = [openstack_networking_subnet_v2.opsathlan_subnet] } resource \u0026#34;openstack_compute_floatingip_associate_v2\u0026#34; \u0026#34;floating_ip_association\u0026#34; { floating_ip = openstack_networking_floatingip_v2.floating_ip.address instance_id = openstack_compute_instance_v2.nova_instance.id } Ansible를 활용한 환경 구성 및 컨테이너 실행 Ansible를 활용하여 컨테이너 이미지를 빌드하고 실행시킬 수 있는 환경을 구성해야 합니다. 그래서 총 4개의 role를 만들었습니다.\njava 설치 docker 설치 springboot 이미지 빌드 및 수행 cAdvisor 실행 🤔 여담\nJenkins와 같이 자동화 도구를 사용할 수 있지만 초반에는 불필요하다고 생각했습니다. 배포하고자 하는 애플리케이션의 규모는 현저하게 작았고, 난이도가 있어 잘못하면 예상보다 시간을 많이 할애할 수 있다고 판단했습니다.\nspring boot application 배포 application 소개 배포 애플리케이션은 간단하게 hello world만 반환하는 웹 서비스입니다.\n1 2 3 4 5 6 7 8 @RestController public class EchoController { @GetMapping(\u0026#34;/echo\u0026#34;) public String echo() { return \u0026#34;Hello, World!\u0026#34;; } } 빌드 및 실행 Ansible를 활용하여 spring boot application 컨테이너를 실행합니다. 이때 컨테이너는 8080 외부 포트로 열어둡니다.\n순서\nRepository clone Spring boot application build Container Image build Docker 실행 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 --- - name: Clone Spring Boot application repository git: repo: \u0026#34;https://github.com/S0okJu/OpsAthlan.git\u0026#34; dest: /home/ubuntu/app/spring-boot-app version: sc/1 - name: Build Spring Boot application using Gradle command: ./gradlew clean build args: chdir: /home/ubuntu/app/spring-boot-app/scenario_01/spring_echo register: gradle_build failed_when: gradle_build.rc != 0 - name: Check if JAR file exists stat: path: /home/ubuntu/app/spring-boot-app/scenario_01/spring_echo/build/libs/ register: jar_dir - name: Fail if JAR file does not exist fail: msg: \u0026#34;JAR file was not built successfully.\u0026#34; when: jar_dir.stat.exists == False or jar_dir.stat.isdir == False - name: Copy Dockerfile for Spring Boot copy: src: Dockerfile dest: /home/ubuntu/app/spring-boot-app/scenario_01/spring_echo/Dockerfile - name: Build Docker image for Spring Boot application docker_image: name: echo-springboot-app tag: latest source: build build: path: /home/ubuntu/app/spring-boot-app/scenario_01/spring_echo - name: Run Spring Boot container docker_container: name: sc1-springboot-app image: echo-springboot-app:latest state: started ports: - \u0026#34;8080:8080\u0026#34; 자세한 코드는 github 참고 바랍니다.\ncAdvisor cAdvisor는 컨테이너 이미지를 다운받고 컨테이너를 실행할 수 있도록 구성합니다. 컨테이너는 8081로 외부포트를 열어 개발자가 대시보드를 확인할 수 있도록 설정합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- - name: Pull cAdvisor image docker_image: name: google/cadvisor:latest source: pull - name: Run cAdvisor container docker_container: name: cadvisor image: google/cadvisor:latest state: started ports: - \u0026#34;8081:8080\u0026#34; volumes: - \u0026#34;/:/rootfs:ro\u0026#34; - \u0026#34;/var/run:/var/run:ro\u0026#34; - \u0026#34;/sys:/sys:ro\u0026#34; - \u0026#34;/var/lib/docker/:/var/lib/docker:ro\u0026#34; 컨테이너 지표 모니터링 cAdvisor 접속 {floating_ip}:8081에 접속합니다. container 창에 들어가 springboot application을 확인합니다.\nspringboot application에 대한 기본적인 지표를 확인할 수 있습니다.\nTest 자체적으로 만든 test 도구인 gonetworker를 활용하겠습니다.\nGoNetWorker\n자체적으로 만든 네트워크 테스트 도구입니다. 서버에 request를 연속에서 요청한다는 특징이 있습니다.\n블로그 작성 기준 GoNetWorker는 동시로 전송하는 기능이 없어 순차적으로 request를 요청합니다. request 요청 이후 무작위로 sleep합니다. 연속적으로 리소스를 사용하지 않을 것이라고 예상할 수 있습니다.\nNetwork 네트워크 처리율을 보면 처리가 되었다가 0byte로 떨어졌다가 다시 상승합니다. 이는 저희가 사용한 도구인 GoNetWorker의 특징 때문에 발생한 그래프입니다.\n그 이외에도 아래와 같은 특징을 알 수 있습니다.\n평균적으로 1200bytes를 넘어가지 않는다. Memory 그에 비해 memory는 점진적으로 상승하는 그래프를 보여주고 있습니다. 그래프를 보니 웹 애플리케이션이 불필요한 메모리 자원을 낭비한다고 생각했습니다.\n원인을 구체적으로 파악하기 위해서는 springboot에 대한 메트릭이 필요합니다. cAdvisor 만으로는 정교한 메트릭 수집에 한계가 있으므로 프로메테우스를 활용해야 할 것 같습니다.\n다음편에는 \u0026hellip; 다음 편에는 프로메테우스를 활용하여 springboot 관련 메트릭스를 수집하고 문제를 해결해 보겠습니다.\n","date":"2024-09-19T00:00:00Z","permalink":"https://s0okju.github.io/p/opsathlan-1/","title":"OpsAthlan 1 - Simple echo server"},{"content":"개인적으로 운영 모니터링을 공부하고 싶었다. 문제는 클라이언트가 없다는 것이었다. 대안으로 스트레스 테스트 도구를 사용해서 극한의 상황을 확인할 수 있다. 나는 현실적인 상황을 재현하고 싶었다. 그래서 사용자가 직접 내 서버를 사용한 것처럼 패킷을 보내는 도구를 만들기로 결심했다.\nGoNetWorker GoNetWorker는 Go를 활용한 가상 운영 시뮬레이션 도구이다.1 미리 설정한 endpoint 정보를 기반으로 서버에 랜덤으로 request를 보내는 도구이다.\nendpoint 정의 - works.json 테스트를 하기 위해서는 endpoint에 대한 정보를 정확하게 기입해야 한다.\nsettings request를 보내기 위한 설정 정보 works request를 보낼 대상에 대한 정보 tasks url의 경로 정보 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;settings\u0026#34;: { \u0026#34;sleep_range\u0026#34;: 5 // request 전송 시 대기시간 범위 }, \u0026#34;works\u0026#34;: [ { \u0026#34;uri\u0026#34;: \u0026#34;http://localhost\u0026#34;, \u0026#34;port\u0026#34;: 8080, \u0026#34;tasks\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;/users/1\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34; } ] } ] } 구조 GoNetWorker는 사용자가 특별한 행동을 취하기 전까지 지속적으로 request를 보낸다. 초기에는 ctrl + c를 클릭했을 경우 실행이 중단될 수 있도록 구현했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 go func() { for { select { case \u0026lt;-stopChan: fmt.Println(\u0026#34;Received interrupt signal. Shutting down...\u0026#34;) return default: work := config.Works[rand.Intn(len(config.Works))] task := work.Tasks[rand.Intn(len(work.Tasks))] wg.Add(1) client := \u0026amp;http.Client{} go makeRequest(work, task, client, \u0026amp;wg, config.Settings.SleepRange) randomSleep := time.Duration(rand.Intn(config.Settings.SleepRange)) * time.Second time.Sleep(randomSleep) } } }() 세부적인 구조는 아래와 같다.\n예시 1 go run main.go GoNetWorker로 실행되면 아래와 같이 랜덤으로 request를 보내게 된다.\n서버에서 확인하면 아래와 같이 정상적으로 request를 받았음을 알 수 있다.\n보완할 점 초기 버전이라 추가해야할 점이 많다. 다음 글에서는 아래의 기능을 추가하고 찾아오겠다.\nGet으로 전송할 경우 보완 path에 식별자가 있을 경우 parameter가 있을 경우 Post로 전송할 경우 go를 빌드하기 위한 makefile 이렇게 설명하는게 맞는지 모르겠다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-09-15T00:00:00Z","permalink":"https://s0okju.github.io/p/gonetworker-1/","title":"GoNetWorker - 1. GoNetWorker 소개와 초기 기능"},{"content":"Kubernetes 설치는 변재한님의 블로그를 참고했습니다.\nAnsible Ping 문제 Ansible Ping 수행 시 모든 노드에 아래와 같은 오류가 발생했다. 에러 메세지를 확인해보니 ssh 키가 제대로 적용되지 않은 것처럼 보였다.\n1 2 3 4 5 Kubernetes-k8s-node-nf-2 | UNREACHABLE! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Data could not be sent to remote host \\\u0026#34;10.0.2.176\\\u0026#34;. Make sure this host can be reached over ssh: kex_exchange_identification: Connection closed by remote host\\r\\nConnection closed by UNKNOWN port 65535\\r\\n\u0026#34;, \u0026#34;unreachable\u0026#34;: true } Ansible은 ssh를 활용하여 target 노드와 통신한다.\n해결책 1. 공개키 알고리즘 변경 openstack log를 확인해보니 debian image에 공개키가 authorized_keys에 제대로 복사되지 않아 ssh 통신을 제대로 수행하지 못한 것이었다.\nssh 키를 ed25519 알고리즘으로 생성하면 키가 제대로 복사되지 않았지만 rsa로 변경하면 아래와 같이 키가 복사된다.\n보안을 고려하면 rsa보다는 ed25519 알고리즘이 권장된다.1\n다시 ping을 수행하면 제대로 수행된다.\n해결책 2. ssh-agent 실행 만약에 원격으로 openstack 서버에 접속한다고 가정해보자. 사용자를 변경하거나 세션을 한번 끊게 되면 동일한 이유로 문제가 생긴다.\n우선 우리가 초기에 ansible를 세팅할 때 ssh-agent를 실행하고 private key를 추가하는 작업을 한다. ssh-agent는 ssh-add 명령어를 활용하여 키를 기억하고 사용하게 된다.\n1 2 3 4 # 백그라운드에 agent 실행 eval \u0026#34;$(ssh-agent -s)\u0026#34; # 키 추가 ssh-add ~/.ssh/id_rsa ssh-agent는 프로세스로써 작동돼 중간에 ssh가 중단되더라도 프로세스가 남아있다. 그러므로 현재 사용자(stack)가 실행시킨 프로세스를 먼저 없애고 ssh-agent를 실행시키도록 구현하면 된다.\nopenstack은 stack 사용자가 관리한다. stack 사용자의 ~/.bashrc 에 ssh-agent를 추가하여 stack 사용자로 로그인 성공할때마다 실행할 수 있도록 한다.\n1 2 3 4 5 6 7 8 9 # Ansible ssh agent ## Delete used ssh-agent if [ -f ~/scripts/delete_ssh_agent.sh ]; then . ~/scripts/delete_ssh_agent.sh fi ## Create new ssh-agent eval $(ssh-agent -s) ssh-add ~/.ssh/id_rsa.kubespray delete_ssh_agent.sh 은 openstack을 관리하는 사용자(stack)가 가지고 있는 ssh-agent를 삭제하는 스크립트이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash # Get all PIDs of ssh-agent processes owned by the user \u0026#39;stack\u0026#39; pids=$(pgrep -u stack ssh-agent) # Check if any PIDs were found if [ -z \u0026#34;$pids\u0026#34; ]; then echo \u0026#34;No ssh-agent processes found for user \u0026#39;stack\u0026#39;.\u0026#34; exit 1 else # Loop through each PID and attempt to kill the process for pid in $pids; do echo \u0026#34;Killing ssh-agent process with PID $pid...\u0026#34; kill \u0026#34;$pid\u0026#34; sleep 1 # Check if the process was successfully killed if pgrep -u stack -x ssh-agent \u0026gt; /dev/null; then echo \u0026#34;Failed to kill process with PID $pid. You may need to use sudo.\u0026#34; else echo \u0026#34;Successfully killed process with PID $pid.\u0026#34; fi done fi 새롭게 접속하면 아래와 같이 정상적으로 실행되는 것을 알 수 있다.\n이 방식은 단일 사용자만 관리한다는 가정하에서 만든 해결책이다.\nNameserver 문제 playbook를 배포해보자.\n1 ansible-playbook --become -i inventory/test-cluster/hosts cluster.yml nameserver로 인해 문제가 발생했다. 보통 이런 문제는 nameserver의 설정이 잘못된 경우에 발생되며 안전한 google dns(8.8.8.8, 8.8.4.4)로 설정하면 해결된다.\n1 \u0026#34;msg\u0026#34;: \u0026#34;E: Failed to fetch http://security.debian.org/debian-security/pool/updates/main/p/python-apt/python-apt-common_1.8.4.3_all.deb Temporary failure resolving \u0026#39;deb.debian.org\u0026#39;\\nE: Failed to fetch http://security.debian.org/debian-security/pool/updates/main/p/python-apt/python3-apt_1.8.4.3_amd64.deb Temporary failure resolving \u0026#39;deb.debian.org\u0026#39;\\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\u0026#34;, 해결책 1. resolv.conf 수정(잘못된 접근) DNS를 설정하는 resolv.conf 를 확인해보면 127.0.0.53로 설정되어 있다. 안전하게 google dns 서버로 변경해보자\n1 2 3 # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 127.0.0.53 실제로는 우리가 직접 변경하면 안된다.\nDO NOT EDIT THIS FILE BY HAND \u0026ndash; YOUR CHANGES WILL BE OVERWRITTEN\nresolv.conf은 /run 폴더 내에 심볼링 링크가 되어 있기 때문에 마음대로 수정하는 것은 권장하지 않는다.2\n실제로 virt-customize 를 활용해서 resolv.conf를 수정하는 이미지를 별도로 만들었지만 의미가 없었다.\n해결책 2. Terraform 수정 가장 안전한 접근법이다. 네트워크 토폴로지를 보면 쿠버네티스 노드들이 kubernetes-network subnet를 활용한다.\nOpenstack의 subnet에 DNS nameserver를 명시할 수 있다. 그러므로 Openstack 인스턴스를 배포하는 terraform 파일을 수정한다. ~/kubespray/contrib/terraform/openstack/modules/network/main.tf 내에 dns_nameservers 를 google dns로 변경시킨다.\n1 2 3 4 5 6 7 8 resource \u0026#34;openstack_networking_subnet_v2\u0026#34; \u0026#34;k8s\u0026#34; { name = \u0026#34;${var.cluster_name}-internal-network\u0026#34; count = var.use_neutron network_id = openstack_networking_network_v2.k8s[count.index].id cidr = var.subnet_cidr ip_version = 4 dns_nameservers = [\u0026#34;8.8.8.8\u0026#34;,\u0026#34;8.8.4.4\u0026#34;] } 원래라면 variables.tf를 수정해야 했지만 제대로 적용이 되지 않았고, google dns server는 거의 불변에 가깝기 때문에 main.tf에 직접 작성해도 된다고 판단했다.\n아래와 같이 제대로 적용된다.\n그 이외의 debian 문제들 그 이후부터 debian에는 잡다한 문제가 발생했다.\n해결책 1. 클라우드 이미지를 ubuntu로 변경 클라우드 이미지를 debian에서 ubuntu로 변경했다. ssh 키를 ed25519 알고리즘으로 적용해도 정상적으로 작동된다. Ansible playbook를 작동시키면 정상적으로 kubernetes가 배포된다.\nhttps://dev.to/ccoveille/how-to-generate-a-secure-and-robust-ssh-key-in-2024-3f4f\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://askubuntu.com/questions/351168/diffrence-between-the-dns-setting-in-etc-resolv-conf-and-etc-network-interfaces\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-08-17T00:00:00Z","permalink":"https://s0okju.github.io/p/openstack-kubernetes-installation-2/","title":"Openstack - Kubespray를 활용한 Kubernetes 설치 시 Ansible 오류"},{"content":"Openstack 설치 im2sh.log velog을 참고해서 Devstack을 활용해 Openstack을 설치하면 된다. 다만 로컬 환경에서만 수행할 예정이므로 가상 public ip 대역 생성은 제외했다.\n이미지 생성 Nova에 올린 이미지를 생성한다. Ubuntu보다 Debian이 가볍고 커스텀하기 용이해서 선택1하게 되었다. 아래의 명령어는 debian 공식 홈페이지를 참고했다.\n1 2 3 4 5 6 7 wget https://cloud.debian.org/cdimage/cloud/OpenStack/current-10/debian-10-openstack-amd64.qcow2 openstack image create \\ --container-format bare \\ --disk-format qcow2 \\ --file debian-10-openstack-amd64.qcow2 \\ debian-10-openstack-amd64 dashboard를 보면 정상적으로 이미지가 업로드된 것을 알 수 있다.\nKubespray를 활용한 Kubernetes 구축 OS - Ubuntu 24.04 Openstack - 2024.1 Kubespray - v2.21.0 Terraform - v1.9.3 전반적인 설치 과정은 변재한님의 velog를 참고했습니다.\nopenstack v2, v3 API는 다른점이 많습니다. 만약에 v2 버전으로 하고 싶다면 운영체제와 kubespray, terraform 버전을 맞추는 것이 좋습니다.\nssh 통신을 위한 공개키 생성 쿠버네티스 환경 설정을 위해 Ansible를 사용한다. Ansible은 ssh를 활용하여 대상에 접속할 수 있다. 이 키는 추후 Kubernetes에서 노드끼리 통신할 수 있는 openstack의 keypair로써 활용된다.\n1 2 3 4 5 ssh-keygen -t ed25519 -N \u0026#39;\u0026#39; -f ~/.ssh/id_rsa.kubespray eval $(ssh-agent -s) ssh-add ~/.ssh/id_rsa.kubespray 오류 - One of auth_url or cloud must be specified 결론부터 말하자면 클라이언트와 관련된 환경 변수가 저장되지 않아서 생긴 일이다.\n해결 과정 아래의 오류를 읽어보면 openstack의 provider를 정의하는 tf파일에 속성이 정의되어 있지 않다고 한다.\n~/kubespray/contrib/terraform/openstack/version.tf 를 보면 provider가 정의되어 있다.\n공식 홈페이지를 보면 설정값을 추가할 수 있는데 전부다 선택사항이지만 Openstack 설정과 관련 환경변수가 반드시 저장되어 있어야 한다. 그렇지 않으면 terraform에 별도로 설정해야 한다.\n1 2 3 4 5 6 7 8 # Configure the OpenStack Provider provider \u0026#34;openstack\u0026#34; { user_name = \u0026#34;admin\u0026#34; tenant_name = \u0026#34;admin\u0026#34; password = \u0026#34;pwd\u0026#34; auth_url = \u0026#34;http://myauthurl:5000/v3\u0026#34; region = \u0026#34;RegionOne\u0026#34; } 즉 위와 같은 문제가 발생한 것은 클라이언트와 identity 서비스가 상호 작용을 위한 환경 변수가 없어서 생긴 일인 것이다.\n다만 수동으로 terraform 파일에 작성하거나 환경변수를 설정하게 되면 Domain 정보가 부정확하다는 에러 메세지를 마주할 수 있다. openstack 버전에 따라서 지원하는 환경 변수도 다르거나, 같은 조건에 사람마다 실행 가능 여부가 다른 등 복합적인 이유가 존재한 것 같았다.2\n해결책 Openstack에서 제공해주는 클라이언트 환경 스크립트(openrc)를 사용하면 된다.\nhorizon에 사용자를 클릭하면 openstack rc 파일이 있는데, 이를 다운로드해서 실행시키면 된다.\nLinux에서 환경 변수는 bash를 exit하는 순간 사라진다.3 컴퓨터가 한번 꺼지게 되면 openrc 스크립트를 계속 실행시켜줘야 한다. 환경변수를 영속적으로 저장하는 방법은 ~/.bashrc를 설정하는 것이다. 다운로드한 rc 파일을 실행시키도록 파일을 설정한다.\n1 2 3 if [ -f ~/rc/k8s-openrc.sh ]; then . ~/rc/k8s-openrc.sh fi 중간에 비밀번호를 입력 받아야 하므로 openrc 스크립트에 비밀번호를 넣는다. 개인적으로 이런 방법은 보안 측면에서는 좋지 않아 보인다.\n1 2 3 4 5 export OS_USERNAME=\u0026#34;k8s\u0026#34; # With Keystone you pass the keystone password. # echo \u0026#34;Please enter your OpenStack Password for project $OS_PROJECT_NAME as user $OS_USERNAME: \u0026#34; # read -sr OS_PASSWORD_INPUT export OS_PASSWORD=\u0026#34;yourpassword\u0026#34; source ~/.bashrc를 수행하면 아래와 같이 환경 변수가 정상적으로 설정된다.\n다시 terraform으로 kubernetes를 설치하면 정상적으로 실행된다.\nhttps://www.quora.com/Which-Linux-distribution-is-best-suited-for-a-cloud-server-environment-and-why\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/terraform-provider-openstack/terraform-provider-openstack/issues/267\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://askubuntu.com/questions/395212/why-environment-variable-disappears-after-terminal-reopen\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-08-01T00:00:00Z","permalink":"https://s0okju.github.io/p/openstack-kubernetes-installation-1/","title":"Openstack - Kubespray를 활용한 Kubernetes 설치 시 Terraform 오류"},{"content":"서버 상태 지금까지 Multipass 를 활용하여 쿠버네티스 환경을 만들고, 실제로 적용하는 과정을 거쳤다. 현재 3개의 노드로 구성되어 있으며, nfs, mysql, mongo와 같이 데이터베이스가 있는 인스턴스는 nfs, 젠킨스 서버는 ops 인스턴스에 저장했다.\n왜 데이터베이스를 쿠버네티스의 statefulset으로 정의하지 않고 별도로 설치했는지 궁금할 것이다. 이유는 최대한 클라우드 스토리지처럼 구현하고 싶어서였다.\n대시보드 확인하기 일단 쿠버네티스 내에서는 외부와 통신할 수 있는 기능은 크게 두 가지이다.\nIngress Nodeport 일단 사설 네트워크 내에서만 사용할 예정이므로 Nodeport 방식을 사용하도록 하겠다.\n예제 - Grafana ui 확인하기 예로 프로메테우스를 사용한다고 가정해보자. grafana 대시보드에 접근할 필요성이 있을 것이다.\ngrafana ui와 관련된 애플리케이션의 service type를 nodeport로 변경해준 후에 Host server(192.168.50.27)에 방화벽을 설정하면 192.168.50.27 주소로 Grafana ui에 접근할 수 있다. 그러나 문제도 존재한다. 서버에 문제가 생기면 재배포하게 되는데, 쿠버네티스 스케줄러에 따라서 다른 노드에 배치될 수 있다. 이럴 경우 배치된 노드를 확인해가며 기존의 방화벽을 닫고, 새로운 방화벽을 열어줘야 한다.\n쿠버네티스에서는 여러 스케줄링 방식이 있는데1, 그 중 간단한 nodeSelector 로 node1에 강제 배치하여 정적으로 사용할 수 있도록 구성했다.\nnodeSelector 설정 node1에 label를 type=ui로 설정한다.\n1 kubectl label nodes node1 type=ui grafana ui service type 변경 Prometheus, Grafana의 원활한 설치를 위해 kube-prometheus helm을 사용했다. helm 차트의 특성상 values.yaml를 설정하여 값을 주입하므로 values.yaml 를 확인해야 한다. kube-prometheus helm 차트는 여러 helm 차트로 구성되어 있어, 설정하고 싶은 서비스의 values.yaml를 설정하면 된다.\nkube-prometheus-stack/charts/grafana/values.yaml 파일에 대략 216번째 줄에 아래와 같은 서비스 타입을 Nodeport로 변경한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service). ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it. ## ref: http://kubernetes.io/docs/user-guide/services/ ## service: enabled: true # type: ClusterIP type: NodePort # Custom loadBalancerIP: \u0026#34;\u0026#34; loadBalancerClass: \u0026#34;\u0026#34; loadBalancerSourceRanges: [] port: 80 targetPort: 3000 nodePort: 30060 # Custom helm nodeSelector 설정 kube-prometheus-stack/charts/grafana/values.yaml 의 314번째 줄에 node1에 배치되도록 설정했다.\n1 2 3 4 5 ## Node labels for pod assignment ## ref: https://kubernetes.io/docs/user-guide/node-selection/ # nodeSelector: type: ui helm으로 배포한 후 pod 리스트를 보면 {helm으로 배포한 이름}-grafana-{문자열} 보이는데, 파드 내부에는 grafana와 관련된 컨테이너가 포함되어 있다. 즉, 우리가 찾던 ui도 grafana pod 내부에 있다는 것이다.\ngrafana 파드의 정보를 보면 nodeSelector가 정상적으로 적용되었음을 알 수 있다.\n방화벽 설정 node1 인스턴스의 nodeport(30060)에 트래픽이 들어올 수 있도록 허용한다.\n1 2 sudo iptables -t nat -I PREROUTING -i enp3s0 -p tcp --dport 30060 -j DNAT --to-destination 10.120.52.23:30060 sudo iptables -I FORWARD 1 -p tcp -d 10.120.52.23 --dport 30060 -j ACCEPT 30060 포트로 enp3s0 인터페이스에 들어오는 트래픽을 10.120.52.23:30060로 리다이렉션한다. 10.120.52.23:30060에 접속하는 트래픽을 허용한다. 서버의 IP로 접속하면 그라파나 대시보드를 확인할 수 있다.\n문제점 Multipass로 인스턴스를 만들고, 외부와의 접속을 위해 트래픽을 제어하는 것은 문제점이 있다.\n외부와의 접속을 할때마다 iptable를 설정해야 한다.\n서버가 shutdown이 될 경우 이전에 설정했던 iptables rule이 사라진다. 이럴 경우 규칙을 다시 설정하거나, 이를 대비해서 별도의 패키지를 설치해서 iptables이 영속성이 있도록 설정2해야 한다. 인스턴스가 많으면 많을수록 관리하기 어렵다.\n쿠버네티스 이외에도 다양한 인스턴스가 있지만 일리리 명령어로 상태를 확인하고, 설정하는 것은 번거롭다. 그러므로 오케스트레이션 도구가 필요했다. 위의 해결책으로 OpenStack 서버를 구축하고자 한다. 인스턴스를 public cloud 처럼 자유자재로 생성 및 삭제할 수 있으며, 네트워크 설정이 용이했다. 또한 클라우드에 사용되는 기술을 대부분 활용할 수 있어 클라우드를 알아가는데 좋은 도구라고 생각했다.\n다음 포스팅은 기존의 서버를 포맷시키고, openstack 서버 구축기로 찾아오겠다.\nhttps://kubernetes.io/ko/docs/tasks/configure-pod-container/assign-pods-nodes/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://unix.stackexchange.com/questions/52376/why-do-iptables-rules-disappear-when-restarting-my-debian-system\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-07-16T00:00:00Z","permalink":"https://s0okju.github.io/p/server-access-dashboard-and-management-problems/","title":"서버 구축기 - 3. 쿠버네티스에서 대시보드 접속하기 그리고 운영의 문제점"},{"content":"들어가며 쿠버네티스에서 유상태 애플리케이션을 실행할때 주로 PV, PVC를 활용해서 데이터를 저장한다. 위치에 따라 내부, 외부 저장 유무를 결정하는데, 내부적으로 저장하는 것은 접근성도 떨어질 뿐만 아니라 노드를 운용하는데 비효율적이라고 판단했다. 외부적인 방안은 네트워크를 통한 데이터 저장이다.\n그렇다면 온프로미스 환경에서 네트워크로 통신하여 저장할 수 있는 방법이 있을까? 직접 쿠버네티스 환경에 구축하여 적용하고자 한다.\nNFS(Network File System) 네트워크에 파일을 저장하는 방식이다. 즉, 원격 컴퓨터에 있는 파일 및 디렉토리에 엑세스하고, 해당 파일 디렉토리가 로컬에 있는 것처럼 사용하는 분산 파일 시스템이다.\n파일 시스템 중에 ZFS가 있다. 중간에 두 개의 개념이 혼용돼서 NFS에 ZFS 파일 시스템을 설치했다. 그리고 다시 포맷팅했다.. 자세히 알고 싶으면 토스 블로그, OpenZFS로 성능과 비용, 두 마리 토끼 잡기 참고하길 바란다.\n용량 부족 문제 유상태 애플리케이션은 무엇이 있을까? 대부분 데이터베이스를 꼽을 것이다. 데이터베이스는 생각보다 큰 용량을 차지한다. 자체 서버는 SSD 1TB용량을 가지고 있다. 그러나 4개의 노드 인스턴스와 시스템 데이터, 미래에 추가할 인스턴스까지 더하면 많은 용량이 요구된다. 개인적으로 SSD라는 비싼 저장소를 단순 데이터 보관용으로 사용하기에는 많이 아깝다고 생각했다.\n다행히 DAS와 4TB HDD가 있었다. SSD에는 시스템에 활용될 데이터를 저장하고, 그 외의 데이터는 DAS에 저장하도록 구성하려고 한다.\n구성 가상 환경을 하나 만들어서 NFS 관련 패키지를 설치하고, 각각의 노드가 네트워킹을 통해 접속할 수 있게 구성할 예정이다.\n구성도 구성을 간략하게 설명하자면 아래와 같다.\nNFS 서버 전용 가상 머신을 만든다. Host 서버에 DAS를 마운팅하고, NFS Server 인스턴스에 host에 마운팅된 경로를 다시 마운트한다. 환경 설정하기 Disk Format DAS에 있는 HDD는 4TB로 1TB, 1TB, 1.8TB로 파티셔닝했다.\n디스크 포맷 및 파티셔닝은 무중력 인간님의 블로그를 참고하자.\n부팅될때마다 자동 마운팅이 될 수 있도록 fstab 파일을 수정했다.\n문제1. UUID가 없는 경우 ext4로 포맷이 안된경우 PARTITION UUID만 있다. 그러므로 mkfs.ext4 를 활용하여 ext4를 포맷시키자.\n문제 2. Emergency Mode fsatab를 잘못 설정하면 Linux는 최소한의 권한만 제공하는 emergency mode로 도입하게 된다. fstab 파일을 적절하게 기입하자.\nNFS 설치하기 NFS Multipass mounting 1 multipass mount /mnt/das/vol1 nfs:/nfs/vol1 NFS Server 설치 NFS multipass Instance 내에 NFS 서버 라이브러리를 설치하자. 설정은 Jung-taek Lim님의 블로그를 참고했다.\n1 2 3 sudo apt-get install nfs-common nfs-kernel-server rpcbind portmap systemctl enable nfs-kernel-server systemctl start nfs-kernel-server 네트워크 범위를 10.120.52.0/24로 지정했는데, 이는 multipass instance가 10.120.52.0/24 범위 내에 있기 때문이다.\nmulitpass에서 사용되는 네트워크 드라이브를 보면 qemu를 사용한다.\n네트워크 인터페이스를 보면 사용가능한 IP 범위는 10.120.52.0/24 임을 알 수 있다.\n1 2 3 4 5 6 7 sudo mkdir -p /nfs/vol1 sudo chown nobody:nogroup /nfs/vol1 # nfs가 공유할 수 있는 네트워크 설정 sudo vim /etc/exports/nfs/vol1 10.120.52.0/24(rw,sync,no_subtree_check,no_root_squash) # 위의 설정 반영 sudo exportfs -r 마스터 노드에 showmount하여 NFS가 제대로 작동되었는지 확인했을때 아래와 같이 export list가 잘 보여지면 된다.\nDynamic Provisioner 쿠버네티스에서 스토리지를 사용하는 방식을 생각해보자. 전역적인 데이터 스토리지에 접근하기 위해서는 [[D K8S- Pod|Pod]]를 생성할때 PVC를 생성하여, 관리자가 생성한 PV와 연결하게 된다. 그러나 파드를 생성할때마다 PV, PVC를 만들어서 스토리지에 접근해야 한다. 이와 같이 번거로움을 해결하고자 자동으로 볼륨을 생성해주는 Dynamic Provisioner를 사용하게 된다.\nProvisioner는 kubernetes-sigs Github에 있는 nfs-subdir-external-provisioner를 사용했다.\n위의 파일을 적절하게 설정하게 test를 실행시키면, 아래와 같이 pvc가 만들어지게 된다. 또한 파드를 삭제하게 되면 자동으로 pvc가 삭제되는 것을 볼 수 있다.\n참고 https://gruuuuu.github.io/cloud/k8s-volume/ https://do-hansung.tistory.com/57 https://1week.tistory.com/114 ","date":"2024-07-14T00:00:00Z","permalink":"https://s0okju.github.io/p/server-setup-nfs-in-multipass-instance/","title":"서버 구축기 - 2. Multipass를 활용한 NFS 구성"},{"content":"사이드 프로젝트를 직접 운영하기 위해 서버를 구축하기로 결심했다. 그래서 기존에 있는 Windows 데스크톱을 ubuntu 20.04 서버 환경으로 세팅했다.\nRTL8125 오류 여기서 문제가 발생했다. LAN 카드 정보 확인 결과, 네트워크 인터페이스가 제대로 인식되지 않았던 것이었다. 랜카드가 RTL8125의 경우에는 별도의 드라이버 설치가 필요했다. 하나의 모순이 생겼다. 우리가 패키지를 설치할때는 네트워크 연결이 필수지만, 네트워크에 문제가 생겨 외부 패키지를 다운받을 수 없던 것이었다. 물론 해결책이 없던 것이 아니였다.\n안드로이드 폰을 이용해 tethering해서 네트워크 연결을 시키고 해당 라이브러리를 설치1 20.04 은 커널 버전을 5.4를 쓰는데 이 버전에서 LAN이 인식이 안돼 5.8 이상 버전을 설치 2 슬프게도 나는 두 방법에 실패하게 되었고 22.04LTS를 설치하게 되었다. 그리고 깔끔하게 성공했다.\n구축 쿠버네티스의 구축 자체는 생각보다 간단하지만 상업 환경에서 안전하고 가용성 있게 지속적으로 관리하는 것은 어렵다. 그래서 자체 호스팅하는 것은 권장하지 않는다고 한다. 3\n그러나 교육의 관점에서는 자체 호스팅해서 사용하는 것은 좋은 선택이라고 생각한다. 퍼블릭 클라우드를 사용하다가 요금을 폭탄맞을 수 있기 때문이다.\nMultipass 선택한 이유 리눅스 기반 가상 머신은 선택지가 생각보다 다양하게 있다. 나는 vagrant, multipass 중에서 고민했다. Multipass를 선택한 이유는 사용하기 쉬워서이다.\nvagrant의 장점은 다양한 이미지를 사용할 수 있다는 점과 Vagrantfile를 통해 자동화 스크립트를 만들 수 있다는 점이었다. 하지만 단점이 존재했는데, 스크립트를 짜는 법을 알아야한다는 점과 내가 쿠버네티스 구축하는 전반적인 방식을 이해하지 못해 디버깅 하지 못한다는 것이었다. 실제로 깃허브의 오픈소스를 clone해서 구성해봤는데, 네트워크 할당이 제대로 되지 않아 많은 시간을 쓰기도 했다.\nmultipass는 Canonical에서 만들었으며 Ubuntu의 가상화환경(VM)을 쉽게 구성할 수 있도록 해주는 도구4이다. 즉 우분투 보급사가 만든 가상환경이라고 보면 된다. 우분투 외의 리눅스를 써야한다면 multipass는 좋은 선택지는 아니지만 centOS Linux가 더이상 지원되지 않는다는 점5에서 쓸 수 있는 건 우분투 밖에 없는 것 같다.\n쿠베네티스 구성 자세한 설치는 enumclass tistory를 참고했다.\nCalico 선택한 이유 CNI 플러그인에서 가장 대중적으로 쓰이는 것은 Calico이다. CNI 플러그인 벤치마크를 보면 리소스 사용률이나, 속도 측면에서 Calico가 우수한 편이다.\nCNI의 특징은 Pod에 IP를 할당한다. 이러한 특징이 대규모 환경에서는 IP 부족으로 이어질 수 있다. 6 그러나 소규모 프로젝트는 많은 pod를 생성하지 않으므로 IP는 부족하지 않을 것이다.\n결과 10.120.52.x IP는 multipass instance의 IP이고, 192.168.x.x는 calico의 IP이다.\n실제로 파드를 생성해 실행시켜 보면 아래와 같은 결과를 얻을 수 있다.\nhttps://physical-world.tistory.com/56\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://romillion.tistory.com/96\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n쿠버네티스를 활용한 클라우드 네이티브 데브옵스, 존 어렌들, 저스틴 도밍거스 지음, 한빛 미디어\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://kim-dragon.tistory.com/176\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://arstechnica.com/gadgets/2020/12/centos-shifts-from-red-hat-unbranded-to-red-hat-beta/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://dobby-isfree.tistory.com/201\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-07-12T00:00:00Z","permalink":"https://s0okju.github.io/p/server-setup-multinode-kubernetes/","title":"서버 구축기 - 1. 쿠버네티스 멀티 노드 환경 구성하기"},{"content":"Multipass를 구동하려고 하는 중 디스크 용량이 꽉찼다는 메세지를 얻게 되었다.\n1 cannot create temporary directory for the root file system: No space left on device 서버에서 사용중인 SSD는 1TB인데 5개의 Multipass 인스턴스로 다 찬다는 것은 말이 되지 않았다.\n실제 하드웨어 상에서 부착된 용량과 ubuntu lvm에서 사용되는 최대 용량이 달랐다. 그러므로 이 용량을 적절하게 조절해야 한다.\n해결 LV의 용량을 VG의 크기만큼 조절한다. Stack Exchange에서 요구한대로 명령어를 사용하면 해결된다.\n1 2 3 4 5 lvm lvm\u0026gt; lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv lvm\u0026gt; exit resize2fs /dev/ubuntu-vg/ubuntu-lv 자세하게 봐보자. ubuntu-vg는 SSD의 용량만큼 적절하게 할당 받았다. 그러나 Alloc PE를 보면 100 GB 밖에 남지 않았다는 것을 알 수 있다.\nlvdisplay로 lv를 확인해보면 ubuntu-lv가 100GB로 할당되었음을 확인할 수 있다.\n여기서 LVM(Logical Volume Manager)의 개념을 알아야 하는데, 파일 시스템에 추상화 계층을 추가하여 논리적 스토리지를 생성할 수 있게 해준다.1\n저자의 환경에서는 Root LV가 100GB로만 할당되어 있어, 하드 디스크의 용량 만큼 사용할 수 없었던 것이었다.\n현재 환경에서는 하나의 물리 디스크만 사용하기 때문에 설정하는데 복잡한 것은 없다. 그러므로 추가적으로 ubuntu-vg에 lv를 추가하지 않는 한 Root lv를 vg 크기만큼 사이즈를 키우면 된다.\nhttps://tech.cloud.nongshim.co.kr/2018/11/23/lvmlogical-volume-manager-1-%EA%B0%9C%EB%85%90/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-05-28T00:00:00Z","permalink":"https://s0okju.github.io/p/server-management-disk-1/","title":"그 많은 서버 디스크 용량을 누가 잡아 먹었는가?"},{"content":"ORM 선택 Go의 경우에는 JAVA와 달리 대표적인 ORM 프레임워크가 없어, 개발자가 직접 선택해서 사용해야 한다.\n다양한 Go ORM 프레임워크 순위를 알고 싶다면 OSS Insight 를 참고하길 바란다.\n기본적으로 mysql driver 사용을 생각했다. 그러나 정교한 저수준의 쿼리를 다루는 것이 아니기 때문에 ORM를 적극적으로 활용하고 싶었다.\n조건 깔끔한 도메인 정의 여러 종류의 DB 지원 컴파일 레벨에서 디버깅 가능 Ent Ent란 Facebook에서 개발한 Go ORM이다. 공식 설명에 의하면 그래프 구조의 데이터베이스 스키마를 가지고 있으며, 코드 생성을 기반으로 하는 정적 타이핑을 지원한다.1 이는 위에서 말한 조건에 어느 정도 충족이 된다.\n다만 ent는 관계형 데이터베이스에 적합하며 NoSQL 기반 데이터베이스에는 적합하지 않았다.2 ORM 선택 조건에는 부합하지 않았으나 대부분의 ORM이 RDB 위주로 지원한다는 것3을 감안했을때 ent은 RDB 사용 시 괜찮은 선택이라고 생각한다.\n초반에 Gorm 사용도 고려했다. 그러나 모델을 정의하는데 사용되는 struct tag 는 개인적으로 가독성이 좋지 않다는 인상이 들었다.\n1 2 3 4 5 6 type Model struct { ID uint `gorm:\u0026#34;primaryKey\u0026#34;` CreatedAt time.Time UpdatedAt time.Time DeletedAt gorm.DeletedAt `gorm:\u0026#34;index\u0026#34;` } 첫인상 ORM 라이브러리의 schema를 본 순간 그래프 데이터베이스인 줄 알았다. 과거에 Neo4j4라는 그래프 데이터베이스를 사용해본 적이 있는데, 이 노드, 그래프를 별도로 정의해서 구현한 점이 상당히 유사했기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 type Task struct { ent.Schema } // Fields of the Task. func (Task) Fields() []ent.Field { return []ent.Field{ field.Int(\u0026#34;id\u0026#34;), field.String(\u0026#34;title\u0026#34;), field.Int(\u0026#34;total_status\u0026#34;), field.Time(\u0026#34;created_at\u0026#34;).Default(time.Now()), field.Time(\u0026#34;modified_at\u0026#34;).Default(time.Now()), } } // Edges of the Task. func (Task) Edges() []ent.Edge { return []ent.Edge{ edge.To(\u0026#34;subtask\u0026#34;, SubTask.Type), } } 공식 홈페이지를 참고해보면 Ent ORM에 대해 이렇게 설명했다.\nEasily model database schema as a graph structure.\nschema 구조를 그래프 구조로 구현되어 있다는 것이다. 아마 ent가 그래프 탐색에 대한 자신감을 표현한 것도 구조적인 이유때문이지 않을까 싶다.\n적용하기 Schema ent의 참조 방식은 독특하다. 기존의 참조 방식과 반대이기 때문이다.5 ent 공식 문서에 의하면 edge.To를 사용하고 있으면 설정한 Edge를 소유한다고 정의한다.6\nA schema that defines an edge using the edge.To builder owns the relation, unlike using the edge.From builder that gives only a back-reference for the relation (with a different name).\n🤔 필자의 경우 위의 정의를 고려하고 구현하니 더 헷갈리기 시작했다. 그래서 관계 소유자인 schema만 정의하고, 그 외에는 서로의 연관관계를 설정해준다는 마음으로 구현했다.\nMember : Task Entity가 1:N 연관 관계를 가진다고 가정해보자.\nMember는 tasks라는 관계의 소유자이다. 그러므로 edge.To로 관계를 설정한다. 하지만 Task는 many에 해당되기 때문에 아무것도 설정하지 않는다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Fields of the Member. func (Member) Fields() []ent.Field { return []ent.Field{ field.Int(\u0026#34;id\u0026#34;), field.String(\u0026#34;email\u0026#34;), field.String(\u0026#34;username\u0026#34;), field.String(\u0026#34;password\u0026#34;), field.Time(\u0026#34;created_at\u0026#34;).Default(time.Now()), } } // Edges of the Member. func (Member) Edges() []ent.Edge { return []ent.Edge{ edge.To(\u0026#34;tasks\u0026#34;, Task.Type), } } Task에서는 Member에서 소유한 관계(user)를 역참조해서 관계를 정의하게 된다. 이때 Member는 관계에서 One에 해당되니 Unique() 함수를 붙이게 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Fields of the Task. func (Task) Fields() []ent.Field { return []ent.Field{ field.Int(\u0026#34;id\u0026#34;), field.String(\u0026#34;title\u0026#34;), field.Int(\u0026#34;total_status\u0026#34;), field.Time(\u0026#34;created_at\u0026#34;).Default(time.Now()), field.Time(\u0026#34;modified_at\u0026#34;).Default(time.Now()), } } // Edges of the Task. func (Task) Edges() []ent.Edge { return []ent.Edge{ edge.From(\u0026#34;member\u0026#34;, Member.Type).Ref(\u0026#34;tasks\u0026#34;).Unique(), } } 참조하는 이유는 어떤 schema와 참조하는지를 명시하기 위해서라고 보면 된다.\n\u0026hellip; because there can be multiple references from one schema to other.\n예제 - 데이터 생성 코드 그럼 데이터를 생성할때 어떻게 해야할까? 참조하는 Schema(Task)에서 Member 정보를 추가해주면 된다.\n공식 문서에서는 직접 Query해서 데이터를 가져왔지만7 , 그 외에도 Schema 데이터( 예제에서는 ent.Member) 혹은 아이디만으로도 추가가 가능하니 공식 문서를 참고하길 바란다.\n1 2 3 4 5 6 7 8 9 func (s *Store) Create(ctx *gin.Context, b request.CreateTask) error { // create Task _, err := s.client.Task.Create().SetTitle(b.Title).SetTotalStatus(0) .SetMemberID(b.UserId).Save(ctx) if err != nil { return err } return nil } 실제로 데이터베이스를 보면 {참조하는 관계명} _ {참조하는 관계명}으로 이뤄져 있다.\n위의 예제에서는 1:N(One-to-Many)인 경우에면 설명했지만 (M:N)의 경우에는 {참조하는 관계명} _ {참조하는 관계명}의 이름을 가진 테이블이 생성된다.\nhttps://entgo.io/docs/getting-started/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n필자는 MongoDB에 적용시키고자 검색을 여러번 했지만 끝내 찾지 못했다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://blog.billo.io/devposts/go_orm_recommandation/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://neo4j.com/docs/getting-started/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://umi0410.github.io/blog/golang/how-to-backend-in-go-db/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://entgo.io/docs/schema-edges#quick-summary\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://entgo.io/docs/schema-edges#o2o-two-types\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-05-22T00:00:00Z","permalink":"https://s0okju.github.io/p/entgo-1/","title":"Entgo - Ent ORM"},{"content":"Go를 활용하여 직접 2~3개의 서비스를 만들고 나니, 구현할때 지나쳤던 에러들이 큰 눈덩이로 변해서 덮쳐왔다. 문제가 생긴 지점은 명확했지만, 그것보다는 문제 발생의 근본적인 원인을 찾고자 했었다. 내가 생각한 가장 큰 원인은 테스트 코드의 부재였다. 테스트의 장점은 누구나 알고 있을 것이다. 코드의 안전성을 확보할 수 있으며 문제 발생 시 빠르게 수정할 수 있다는 것이다. 현재 코드에는 숨겨진 구멍들이 존재하는데, 이를 해결하기 위해서는 단위 테스트, 통합 테스트가 필요했다. 기존에 있던 코드를 활용하여 테스트 코드로 작성하려고 시도했으나, 지금까지 적은 Go 코드는 테스트를 용이하도록 작성되지 않았다. 특히 인터페이스를 적절하게 사용하지 않아서 Mock를 사용하는데 어려움이 생기기도 했다. 그래서 처음부터 다시 제작하고자 한다.\n나는 동일한 코드에 대해 이번까지 포함해서 2번 변경했다. 어떻게 변경했는지 알아보자\n초기 - Go 한달차의 적응기 에러 타입과 상태 타입 1차에서는 ErrorType으로 특정 에러 타입을 정의하는데 사용할 타입을 지정해줬다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type ErrorType int type Error struct { // Code is a custom error code ErrorType ErrorType // Err is a error string Err error // Description is a human-friendly message. Description string } type ErrorRes struct { // Code is http status code Code int `json:\u0026#34;code\u0026#34;` } 모든 서비스들이 사용할 경우, 각 서비스가 사용할 에러 코드를 일리리 지정했다. map를 활용하여 에러 타입에 해당되는 상태 코드를 매핑했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 const ( // Common SUCCESS ErrorType = 0 INVALID_JSON_FORMAT ErrorType = 1001 INVALID_URI_FORMAT ErrorType = 1002 // Member INVALID_MEMBER ErrorType = 2001 ERROR_MEMBER_DB ErrorType = 2101 // Task ERROR_TASK_DB ErrorType = 3101 ) var codeMap = map[ErrorType]int{ // Common INVALID_JSON_FORMAT: http.StatusBadRequest, INVALID_URI_FORMAT: http.StatusBadRequest, // Member INVALID_MEMBER: http.StatusUnauthorized, ERROR_MEMBER_DB: http.StatusInternalServerError, // Task ERROR_TASK_DB: http.StatusInternalServerError, } 에러 반환 직접 커스텀한 에러값을 입력값으로 해서 context를 활용해 Json 형태로 반환해줬다.\n1 2 3 4 5 6 7 8 9 10 11 12 // getCode is get Status code from codeMap. func getCode(flag ErrorType) *ErrorRes { return \u0026amp;ErrorRes{Code: codeMap[flag]} } func ErrorFunc(ctx *gin.Context, err *Error) { res := getCode(err.ErrorType) log.Println(err) ctx.AbortWithStatusJSON(res.Code, res) return } \u0026ldquo;저렇게 하면 오류가 제대로 반환되지 않을텐데?\u0026ldquo;라고 생각하는 사람이 있을 것이다. 저때 당시 gin이 에러를 응답하는 매커니즘을 잘 몰라 단순하게 AbortWithStatusJson으로 응답하도록 했다. 비록 결과는 에러 json + 성공 json 둘다 나왔지만 말이다.\n1차 - Middleware가 있었다고요? gin의 오류 처리 매커니즘을 제대로 이해하지 못해 단순하게 응답을 처리했다. 이제는 Middleware를 제작함으로써 에러 응답 반환이 수월하도록 구현했다.\n에러 상태 타입 ErrorType -\u0026gt; WebCode로 이름을 변경했으며, 에러뿐만 아니라 상태 전체를 표시했다. 여기서 눈치 빠르신 분은 알겠지만 에러 타입에는 상태값을 직접 포함하도록 구현했다. 왜냐하면 상태 코드를 추가할 일이 많은데 작업을 두번해야 하기 때문이다.\n1 2 TaskCreationSuccess WebCode = 220101 SubtaskCreationSuccess WebCode = 220151 예로 220101 이라면 2(Task Service)/201(Status code)/01(Unique number)로 구성하도록 했다.\n에러 타입과 반환 이전에는 에러 타입을 Error라고 정했지만 gin 내부의 에러를 쓰면서 헷갈리기 시작했다. 그래서 NetError로 이름을 변경했으며, Description field는 잘 사용하지 않는 것 같아 삭제했다.\n1 2 3 4 5 6 7 8 9 10 11 12 type NetError struct { Code codes.WebCode Err error } func NewNetError(code codes.WebCode, err error) *NetError { logrus.Errorf(\u0026#34;Code : %d, Error : %v\u0026#34;, code, err) if err != nil { return \u0026amp;NetError{Code: code, Err: err} } return \u0026amp;NetError{Code: code, Err: nil} } 에러는 동일하게 code만 반환하도록 구현했다.\n1 2 3 4 5 6 7 8 // Response type ErrorResponse struct { Code codes.WebCode `json:\u0026#34;codes\u0026#34;` } func NewErrorResponse(code codes.WebCode) *ErrorResponse { return \u0026amp;ErrorResponse{Code: code} } 성공 응답값을 생각해보면 크게 두 가지가 존재한다.\n데이터가 없는 경우 데이터가 있는 경우 이 두가지를 나눠서 별도의 함수를 만들었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 func SuccessWith(ctx *gin.Context, code codes.WebCode, data any) { status := codes.GetStatus(code) res := NewSuccessResponseWith(code, data) ctx.Header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) ctx.AbortWithStatusJSON(status, res) } func Success(ctx *gin.Context, code codes.WebCode) { status := codes.GetStatus(code) ctx.Header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) res := NewSuccessResponse(code) ctx.JSON(status, res) } 에러 반환 로직은 gin Middleware를 사용했다. 자세한 사항은 [[Gin - 예외처리 2. 커스텀 예외처리 구현하기]] 를 참고하길 바란다.\n2차 - 프론트엔드를 하고나니\u0026hellip; 왜 백엔드와 프론트엔드가 짝꿍이 맞아야 하는지 알 것 같았다. 플러터의 경우에는 응답값과 똑같은 필드를 가진 모델로 반환해야 한다. 만약에 정확하게 받지 못하면, 아무런 값도 받을 수 없다.(심지어 상태값도 말이다.) 플러터로써는 해결 방안을 찾지 못해서 서버측에서 성공이든 오류든 모두 동일한 형태로 반환하도록 구현해야 했었다.\nResponse Format 위에서 1차, 2차를 보면 구체적인 메세지가 없었다. 메세지를 넣지 않은 이유는 클라이언트에게 저런것까지 보여줄 필요가 없다고 생각해서였다. 그러나 앱개발하면서 정보가 없어서 정말 힘들었다. 😥\nError Response의 Best Practice를 찾아보니 대부분 자체적인 code와 이를 설명하는 메세지가 포함되어 있었다.1 이번에 수정한 응답에서는 메세지를 추가하기로 했다.\n에러 상태 타입 일단 지금까지 사용했던 에러 코드보다 길이를 더 축소시켰다. 그래서 자주 사용하는 상태 코드를 백의 자리로 두고, 1씩 카운트를 추가하면서 에러 코드를 정의했다. 또한 에러코드에 매핑되는 에러 메세지를 추가함으로써, 동일한 에러 메세지를 반환하도록 했다. 메세지는 정확하게 적지 않고, 문제를 전반적으로 보여줄 수 있도록 작성했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const ( // 1xx is Bad Request error code InvalidHeader ErrorCode = 101 InvalidBody ErrorCode = 102 InvalidQuery ErrorCode = 103 // 2xx is Unauthorized error code BadAuthenticationData ErrorCode = 201 TokenExpired ErrorCode = 202 ... ) var errorMessage = map[ErrorCode]string{ InvalidHeader: \u0026#34;The provided header values are invalid.\u0026#34;, InvalidBody: \u0026#34;The body of the request is invalid.\u0026#34;, InvalidQuery: \u0026#34;The query parameters are invalid.\u0026#34;, ... } 에러 타입 netErrorOptions 라는 옵션을 추가시켰는데, 여기에는 서비스의 메타 정보가 포함되어 있다.\n1 2 3 4 5 6 7 8 9 10 11 // NetError have options and ErrorCode that contains status code. type NetError struct { // options is metadata about service options netErrorOptions // Type is a unique data that contains http status code. Code ErrorCode // Description is an error details. Description string // Err is an error message. Err error } 메타 정보로써 태그가 있는데, 서비스 이름을 문자열로 표시하는 것이다. 태그는 서버에서 디버깅을 조금더 수월하게 하기 위해서 도입했다. 오류코드가 모두 동일해서 어떤 서비스의 오류인지 명시하는 용도로 사용된다.\n1 2 3 4 5 // netErrorOptions is meta data about service type netErrorOptions struct { // tag is the service name tag string } 이 태그는 애플리케이션을 시작할때 한번만 설정하도록 함으로써, 모든 NetError가 해당 tag를 가지도록 구현했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 var ( opts netErrorOptions ) // netErrorOptions is meta data about service type netErrorOptions struct { // tag is the service name tag string } // SetTag sets the service name func SetTag(tag string) { mu := sync.Mutex{} mu.Lock() defer mu.Unlock() opts.tag = tag } 에러 반환 에러는 크게 두 가지로 나눠져 있다.\nStatus: 응답의 타입 (\u0026ldquo;Success\u0026rdquo; or \u0026ldquo;Error\u0026rdquo;) Data : 에러 데이터 혹은 성공 시 반환할 데이터 1 2 3 4 5 // BaseResponse is common response that use in success and failed type BaseResponse struct { Status string `json:\u0026#34;status\u0026#34;` Data interface{} `json:\u0026#34;data\u0026#34;` } 다만 생성자를 만들때 성공, 실패 여부를 따로 만들었다. 그 이유는 입력하는 값이 다 달랐기 때문이다.\n성공한 경우 반환하고자 하는 데이터를 Data 필드에 넣기만 하면 된다.\n1 2 3 4 5 6 func NewSuccessBaseResponse(data interface{}) *BaseResponse { return \u0026amp;BaseResponse{ Status: \u0026#34;Success\u0026#34;, Data: data, } } 그러나 오류인 경우는 다르다. 반환할 오류 타입은 형식화되어 있으므로, ErrorResponse 구조체를 따로 만들어서 Data 필드에 저장했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // ErrorResponse is the error response format type ErrorResponse struct { Code ErrorCode `json:\u0026#34;code\u0026#34;` Message string `json:\u0026#34;message\u0026#34;` } func NewErrorBaseResponse(data NetError) *BaseResponse { res := ErrorResponse{Code: data.Code, Message: GetErrorMsg(data.Code)} return \u0026amp;BaseResponse{ Status: \u0026#34;Error\u0026#34;, Data: res, } } BaseResponse가 반환되면, 메소드로 응답값을 반환하도록 구현했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Failed response failed formatted response. // It converts NetError to ErrorResponse to extract necessary things. func (b BaseResponse) Failed(ctx *gin.Context) { res := b.GetErrorData() status := parseStatusCode(res.Code) ctx.AbortWithStatusJSON(status, b) return } // OKSuccess uses when status code is 200 func (b BaseResponse) OKSuccess(ctx *gin.Context) { ctx.Header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) ctx.AbortWithStatusJSON(200, b) } 테스트 gin 테스트 사례2를 참고해서 성공했을 경우의 테스트 코드를 작성해보았다. 테스트 코드는 테이블 중심 테스트(Table-Driven Test) 방식3을 채택했다. 단위 테스트가 비슷한 구조로 이뤄져 있는 경우, 테이블 중심 테스트로 수행하면 코드 중복을 피할 수 있어 로직을 변경하거나 새로운 케이스를 추가하기 쉽다는 장점이 있다. 작성한 테스트 코드에는 테스트 구조가 비슷하고, 라우팅 설정 등 공통된 부분이 많아서 테이블 중심 테스트를 선택하게 되었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 func setupRouter() *gin.Engine { r := gin.Default() r.Use(errorutils.ErrorMiddleware()) return r } func TestSuccessResponse(t *testing.T) { r := setupRouter() res := errorutils.NewSuccessBaseResponse(nil) // Set table tests := []struct { description string fn func(*gin.Context) expectedCode int path string }{ { // Ok Success description: \u0026#34;OKSuccess\u0026#34;, fn: func(c *gin.Context) { res.OKSuccess(c) }, expectedCode: 200, path: \u0026#34;/test/ok\u0026#34;, }, { // Created Success description: \u0026#34;CreatedSuccess\u0026#34;, fn: func(c *gin.Context) { res.CreatedSuccess(c) }, expectedCode: 201, path: \u0026#34;/test/created\u0026#34;, }, } for _, tc := range tests { t.Run(tc.description, func(t *testing.T) { w := httptest.NewRecorder() req, _ := http.NewRequest(\u0026#34;GET\u0026#34;, tc.path, nil) // Specific handler function and serve r.GET(tc.path, tc.fn) r.ServeHTTP(w, req) if w.Code != tc.expectedCode { t.Errorf(\u0026#34;Expected %d, got %d\u0026#34;, tc.expectedCode, w.Code) } }) } } 테스트 코드를 실행하면 아래와 같은 결과를 얻을 수 있다.\n마치며 하나의 간단한 CRUD가 있는 서비스 2개를 가지고 코드를 여러번 수정했다. 그 과정에서 다양한 오류를 만났고, 테스트 코드의 중요성도 확실히 알았다. 특히 이번에 테스트 코드를 작성하면서 오류를 많이 마주 했는데, 이를 재빠르게 수정하면서 테스트의 중요성을 느끼게 되었다.\nhttps://pjh3749.tistory.com/273\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gin-gonic.com/docs/testing/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://bugoverdose.github.io/development/go-table-driven-tests/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-05-11T00:00:00Z","permalink":"https://s0okju.github.io/p/go-code-refactor-error-1/","title":"Todopoint 서버코드 개선기 -  1. 공동 에러 처리하기"},{"content":"EKS Amazon Web Services(AWS)에 Kubernetes 컨트롤 플레인을 설치, 운영 및 유지 관리할 필요가 없는 관리형 서비스이다1.\nECS vs EKS 둘 다 컨테이너 오케스트레이션이라는 점에서 공통점을 가짐 ECS는 AWS가 만든 자체적인 오케스트레이션 구조로 K8S 구조와 완전 다름 K8S는 오픈소스가 크게 활성화되어 있고, 다양한 플러그를 사용할 수 있음 ECS는 오픈소스가 크게 활성화되어 있지 않음. Auto-scaling 측면에서 EKS는 수동 및 자동 배포가 가능하지만 ECS는 수동으로만 가능하다. 오류 수정하는데 ECS는 전문가가 필요함 📌 자세한 사항은 물통꿀꿀이님의 블로그, [AWS] ECS vs Kubernetes를 참고하길 바람.\n🤔 EKS가 무조건 정답은 아니다. 프로젝트에 따라서는 ECS가 더 적합한 선택지일 수 있다.\nECS 적용 사례 - 밍글 왜 EKS 설치는 어려운가? 우리가 알고 있는 쿠버네티스 구조와 달리 쿠버네티스는 일부 도구만 제공하고, 나머지는 별도의 설치가 필요하다. 이런 이유로 버전 관리가 어려워지기도 한다.\n쿠버네티스 제공 : kube-proxy, kubelet CNCF Graduated Project를 보면 etcd, coreDNS 등을 볼 수 있다.\nEKS 구조 앞서 말했듯이 직접 구축하게 되면, 관리해야 하는 요소들이 많다. 이러한 어려움을 덜어내고자 EKS를 사용할 수 있다. EKS는 관리형 \b서비스로서 Control Plane를 직접 구성하지 않고 K8S를 사용할 수 있다. EKS 특징 EKS CNI를 활용하여 VPC 네트워크 상에서 파드간 통신 가능 IAM을 활용하여 권한 설정 가능 AWS가 가용 영역별 API Server ,etcd 배포하여 고가용성 보장 eksctl를 활용하여 워커노드를 custom할 수 있음. 📌 자세한 사항은 Jaden Park님의 블로그, Amazon EKS 란?를 참고하길 바란다.\nAWS Side Workflow 쿠버네티스의 api 서버를 각 가용영역에 배포 api 데이터, 쿠버네티스의 상태 데이터를 확인하기 위해 etcd를 같이 배포 쿠버네티스에서 오는 call 에 대한 IAM 구성 쿠버네티스 마스터 노드의 오토스케일링 설정 클러스터가 안정적으로 구현하도록 여기에 연결할 수 있는 로드밸런서를 구성 Reference 초보자를 위한 EKS 맛 보기 ECS vs Kubernetes https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/what-is-eks.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-05-01T00:00:00Z","permalink":"https://s0okju.github.io/p/what-is-eks/","title":"AWS - EKS란 무엇인가"},{"content":"Introduction Terraform 코드를 분석하고 있었다. 어디에서는 IAM을 쓰고, 어디에서는 Profile을 쓴다. 과연 그 둘의 차이점은 무엇일까? 알아보도록 하자!\nProfile Amazon Ec2는 IAM role을 가진 profile을 사용하게 된다.\n질문 1. awscli를 통해 profile을 지정할 수 있는데, 이것도 Instance Profile인가? If you use the AWS CLI, API, or an AWS SDK to create a role, you create the role and instance profile as separate actions, with potentially different names. If you then use the AWS CLI, API, or an AWS SDK to launch an instance with an IAM role or to attach an IAM role to an instance, specify the instance profile name. 출처 - 공식 문서\n질문이 잘못되었다. EC2는 해당 IAM Role을 가진 Profile을 사용한다. 그래서 awscli의 경우 \u0026ndash;profile을 통해 직접 profile 이름을 지정할 수 있다. 왜 필요할까? aws cli를 사용할 때 profile 기능을 이용하면 여러개의 자격 증명을 등록하고 스위칭해서 효율적으로 사용할 수 있습니다.\n출처 - https://cloudest.oopy.io/posting/101\n질문 2. IAM Role과의 차이점은? Profile이 필요한 이유는 여러개의 자격 증명을 등록하고 스위칭해서 효율적으로 사용하기 위함이라고 했다. 특징을 봤을때 IAM의 Role과 비슷해 보였다.\nRoles are designed to be “assumed” by other principals which do define “who am I?”, such as users, Amazon services, and EC2 instances An instance profile, on the other hand, defines “who am I?” Just like an IAM user represents a person, an instance profile represents EC2 instances. The only permissions an EC2 instance profile has is the power to assume a role.\n출처 - https://www.quora.com/In-AWS-what-is-the-difference-between-a-role-and-an-instance-profile\nIAM Role은 무엇을 할 수 있는지에 대한 행위를 정의하는 것이라면 Profile Instance는 \u0026ldquo;누가 만들었는지를 정의하기\u0026rdquo; 위함이라면 보면 된다.\nConclusion 내가 이러한 궁금증을 가지게 된 것은 IAM을 한사람에게만 부여된다고 생각했기 때문이었다. 그런 맥락에서는 profile이 필요 없기 때문이다. Reference https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html ","date":"2024-04-16T00:00:00Z","permalink":"https://s0okju.github.io/p/iam-and-profile/","title":"AWS - IAM 그리고 Profile"},{"content":"이전에는 Gin이 어떻게 예외 처리를 하는지 언급했다. 이제부터 직접 구현하고자 한다.\nError Wrapping 직접 제작한 에러 코드인 WebCode와 에러를 감싼 NetError를 만들었다.\n1 2 3 4 type NetError struct { Code codes.WebCode Err error } 프로젝트 구현할때 주로 gin 혹은 ent 라이브러리를 쓴다. 별도의 Error 구조체(NetError)를 정의함으로써 Err에 gin 혹은 ent 에러를 담겠다는 의미인 것이다.\nGin에서의 예외 처리 Gin Error() 함수에 의하면 Gin의 Context에 Error를 담은 후에 Middleware에서 처리하는 것을 권장하고 있다.\nError attaches an error to the current context. The error is pushed to a list of errors. It\u0026rsquo;s a good idea to call Error for each error that occurred during the resolution of a request. A middleware can be used to collect all the errors and push them to a database together, print a log, or append it in the HTTP response. Error will panic if err is nil.\n라이브러리 코드를 보면 Errors 필드가 정의되어 있는데, errorMsg는 []*Error 에러 리스트로 타입을 가지고 있다. 위의 설명대로 error의 리스트가 Context 내부에 구현되어 있는 것이다.\n1 2 3 4 5 6 7 8 9 // gin/context.go type Context struct { // ... Errors errorMsgs // .. } // gin/errors.go type errorMsgs []*Error 즉, ctx.Error를 활용하여 입력받은 에러를 Gin의 에러로 감싼 후에 Context의 Errors 리스트에 넣게 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // gin/context.go // Error attaches an error to the current context. The error is pushed to a list of errors.// It\u0026#39;s a good idea to call Error for each error that occurred during the resolution of a request.// A middleware can be used to collect all the errors and push them to a database together, // print a log, or append it in the HTTP response. // Error will panic if err is nil. func (c *Context) Error(err error) *Error { if err == nil { panic(\u0026#34;err is nil\u0026#34;) } var parsedError *Error ok := errors.As(err, \u0026amp;parsedError) if !ok { parsedError = \u0026amp;Error{ Err: err, Type: ErrorTypePrivate, } } c.Errors = append(c.Errors, parsedError) return parsedError } Context 내부에 있는 Error 리스트는 Middleware에서 처리하게 된다.\nGin에서는 HandlerFunc를 slice로 구현된 HandlerChain이 있는데, 이는 Gin이 각가지의 Handler를 Chain내에 넣고 처리하는 것이다.\n1 type HandlersChain []HandlerFunc 그럼 오류를 어떻게 발생시키면 될까? HandlerChain 내에 있는 대기 중인(Pending) Handler를 호출하지 않도록 하면된다. 즉, Context를 Abort하면 되는 것이다. 그 이후에 처리할 Handler가 없어지면서 종료가 된다.\nAbort prevents pending handlers from being called. Note that this will not stop the current handler. Let\u0026rsquo;s say you have an authorization middleware that validates that the current request is authorized. If the authorization fails (ex: the password does not match), call Abort to ensure the remaining handlers for this request are not called.\n프로젝트에 적용하기 절차 위의 내용을 가지고 실제로 적용해보자. 프로젝트 구성은 3계층으로 커스텀 에러 감싸기는 서비스 로직에 수행하도록 했다.\nService 계층에 Error Wrapping을 한다. Controller에 context 내에 있는 에러 리스트에 예외를 넣는다. Middleware에 Error Wrapping한 것을 Unwrapping 하면서 예외 타입을 학인한다. 커스텀 에러라면 WebCode에 따른 응답값을 반환한다. 그림으로 표현하자면 아래와 같다. 코드 코드는 아래의 두 사이트를 참고했습니다.\nNaver D2, Golang, 그대들은 어떻게 할 것인가 - 3. error 래핑 Naver D2, Golang, 그대들은 어떻게 할 것인가 - 4. error 핸들링 Service Layer Persistence Layer에서 얻은 에러 값을 직접 받은 후에 Service 계층에서 적절하게 NetError로 감싼다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // service func (s *MemberService) CreateMember(ctx *gin.Context, req data.RegisterReq) (*ent.Member, *errorutils.NetError) { // Check member Exist existedMem, err := s.Store.GetMemberByEmail(ctx, req.Email) if err != nil { return nil, \u0026amp;errorutils.NetError{Code: codes.MemberInternalServerError, Err: err} } if ent.IsNotFound(err) { mem, err2 := s.Store.Create(ctx, req) if err2 != nil { return nil, \u0026amp;errorutils.NetError{Code: codes.MemberCreationError, Err: err2} } return mem, nil } return existedMem, nil } Controller Layer Controller는 Service 계층에서 감싼 커스텀 에러를 받은 후에 Context의 에러 리스트에 넣는다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func (controller *MemberController) RegisterMember(ctx *gin.Context) { req := data.RegisterReq{} err := ctx.ShouldBindJSON(\u0026amp;req) if err != nil { _ = ctx.Error(errorutils.NewNetError(codes.MemberInvalidJson, err)) return } // Create member mem, err2 := controller.service.CreateMember(ctx, req) if err2 != nil { // Service 계층에서 받은 에러를 Context 내 에러 리스트에 넣는다. _ = ctx.Error(err2) return } mid := data.MemberId{MemberId: mem.ID} response.SuccessWith(ctx, codes.MemberCreationSuccess, mid) } Middleware 에러 응답값을 반환할 HandlerFunc를 구현한다.\nNext를 활용하여 대기 중인 핸들러를 실행시킨다. errors.As를 활용하여 Context 에러 리스트에 있는 에러가 커스텀 에러인지 확인한다. 정확히 말하자면 에러를 unwrapping하면서 커스텀 에러인지 확인한 후에 있다면 netError에 넣게 된다. WebCode를 활용하여 응답값을 얻은 후 AbortWithStatusJson 를 활용하여 Response json을 전송한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func ErrorHandler() gin.HandlerFunc { return func(ctx *gin.Context) { // Pending 중인 핸들러 실행 ctx.Next() // JSON이 두번 쓰이는 것을 대비해서 Body 확인 isBodyWritten := ctx.Writer.Written() err := ctx.Errors.Last() if err != nil { // 커스텀 에러인지 확인 var netErr *errorutils.NetError if errors.As(err, \u0026amp;netErr) { code := netErr.GetCode() statusCode := codes.GetStatus(code) res := response.NewErrorResponse(code) if !isBodyWritten { ctx.AbortWithStatusJSON(int(statusCode), res) } } else { res := response.NewErrorResponse(codes.GlobalInternalServerError) if !isBodyWritten { ctx.AbortWithStatusJSON(http.StatusInternalServerError, res) } } } } } 마무리 Go의 예외 매커니즘은 다른 언어와 달라서 틀을 잡는데 많은 시간을 사용했다. 공식 문서나 다른 사람들의 예제 코드를 분석하면서, Go 스러움이 무엇인지 자츰 배워간다는 느낌이다. 그러나 코드양이 많아지면서 오히려 코드가 복잡해지는 것 같기도 하다.\n","date":"2024-04-10T00:00:00Z","permalink":"https://s0okju.github.io/p/gin-error-handling-2/","title":"Gin 예외처리 - 2. 커스텀 예외처리"},{"content":"Go와 예외처리 Go에서의 예외처리 Go에서는 함수에서 반환된 에러 객체(error)로 처리한다. 다행히도 multi-return이 가능하기에 에러 반환을 더욱 수월하게 해줄 수 있다.\n1 2 3 4 f, err := Sqrt(-1) if err != nil { fmt.Println(err) } try ~ catch가 없는 이유 공식 문서에 의하면 try ~ catch는 난해한 코드를 생성하며, 개발자에게 너무많은 일반적인 예외를 처리하도록 장려한다고 한다.\nWe believe that coupling exceptions to a control structure, as in the try-catch-finally idiom, results in convoluted code. It also tends to encourage programmers to label too many ordinary errors, such as failing to open a file, as exceptional.\n다른 언어처럼 try ~ catch 를 어렴풋이 구현할 수 있다. 중간에 실행의 흐름을 끊는 panic 함수를 사용하는 것이다. 하지만 반대로 생각하자면 모든 에러들을 panic으로 처리해야 할까? Go에서는 그것이 아니라는 것이다. 이런 이유로 Go는 시의적절하기 예외처리할 수 있도록 error를 반환하는 방식으로 처리하게 된다.\nError - Wrapping Go에서는 에러처리할 때 Error 객체를 넘겨준다고 한다. 물론 일반 에러 객체를 넘겨줄 수 있지만 개발자가 직접 만든 에러를 만들어서 넘겨줄 수 있다. Error Wrapping이란 쉽게 말하자면 error 객체를 감싸는 또다른 구조체를 만드는 것이라고 보면 된다.\ngin에서의 Error를 봐보자. gin의 Error 내에 필드로 error가 존재한다. 이러한 과정을 Error Wrapping이라고 보면 된다.\n1 2 3 4 5 6 // Error represents a error\u0026#39;s specification. type Error struct { Err error Type ErrorType Meta any } 그럼 예를 들어보자. ctx.Error를 실행했는데 의도치 않게 errors.As가 적절하게 실행되지 않는다고 가정해보자. error.As는 Error Type을 확인하는 함수인데, 만약에 타입이 적절하지 않는다면, 입력한 error을 감싼 Error를 반환하게 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (c *Context) Error(err error) *Error { if err == nil { panic(\u0026#34;err is nil\u0026#34;) } var parsedError *Error ok := errors.As(err, \u0026amp;parsedError) if !ok { parsedError = \u0026amp;Error{ Err: err, Type: ErrorTypePrivate, } } c.Errors = append(c.Errors, parsedError) return parsedError } 그럼 원본 에러(error)에 접근할 수 있을까? 바로 Unwrap를 통해 얻을 수 있은 것이다.\n1 2 3 4 // Unwrap returns the wrapped error, to allow interoperability with errors.Is(), errors.As() and errors.Unwrap() func (msg *Error) Unwrap() error { return msg.Err } 그림으로 표현하면 아래와 같다.\n기존의 문제점 Gin Context의 잘못된 활용법 공식 문서에서 말하는 Context는 데드라인, 취소 시그널, API에 대한 경계값을 가지는 값으로 정의된다. 그래서 조건에 따라 실행이 중단될 수 있다는 것으로 이해했다. gin은 자체적인 Context를 가지고 있으며, context를 중단시킬 수 있는 여러 함수들이 존재한다.\nService Layer에서 커스텀 에러 타입으로 반환하도록 구현했다.\n1 2 3 4 5 6 7 8 9 10 11 12 func (controller *MemberController) RegisterMember(ctx *gin.Context, req request.RegisterReq) { req = request.RegisterReq{} err := ctx.ShouldBindJSON(req) // ... // Create member err2 := controller.service.CreateMember(ctx, req) if err2 != nil { errorutils.ErrorFunc(ctx, err2) return } webutils.Success(ctx) } 커스텀 에러 타입을 자세히 보면, 자체적으로 제작한 에러 코드와 error을 담을 Err 필드가 존재한다.\n1 2 3 4 5 6 7 8 type Error struct { // Code is a custom error codes ErrorType ErrorType // Err is a error string Err error // Description is a human-friendly message. Description string } 애플리케이션에 오류 발생시 현재 실행을 멈추고, 응답값을 보내는 ErrorFunc도 만들었다.\n1 2 3 4 5 6 func ErrorFunc(ctx *gin.Context, err *Error) { res := getCode(err.ErrorType) ctx.AbortWithStatusJSON(res.Code, res) return } 공식 문서 에 의하면AbortWithStatusJSON에는 내부적으로 Context를 중단시킬 수 있는Abort 함수를 사용한다. 구체적으로 Abort 함수는 현재의 handler는 그대로 남지만, 그 이후의 handler를 처리하지 않겠다는 것이다.\nAbort prevents pending handlers from being called. Note that this will not stop the current handler. Let\u0026rsquo;s say you have an authorization middleware that validates that the current request is authorized. If the authorization fails (ex: the password does not match), call Abort to ensure the remaining handlers for this request are not called.\n문장을 보면서 내가 Gin 프레임워크의 에러처리를 완전히 잘못했음을 깨닫게 되었다.\nGin Error 미사용 공식 문서에 의하면 Gin은 자신들의 Error를 사용하는 것을 권장하며, middleware가 이를 처리하여 오류 response를 처리하라고 명시되어 있다.\nError attaches an error to the current context. The error is pushed to a list of errors. It\u0026rsquo;s a good idea to call Error for each error that occurred during the resolution of a request. A middleware can be used to collect all the errors and push them to a database together, print a log, or append it in the HTTP response. Error will panic if err is nil.\n즉, 오류가 발생할때마다 gin의 Context에서 제공해주는 Error로 감싸며, Middleware에 있는 Handler가 이를 순차적으로 처리해야 한다는 것이다.\n2부에서는 지금까지는 내가 만들었던 예외처리에는 어떠한 문제점이 있는지 확인해봤다. 2부에서는 위에서 설명한 잘못된 에러처리를 공식문서에서 제시한 올바른 에러처리를 구현하고자 한다.\nMiddleware에 Handler 구현 gin.Error를 활용하여 Error를 wrapping하고, Middleware에서 처리하기 ","date":"2024-04-03T00:00:00Z","permalink":"https://s0okju.github.io/p/gin-error-handling-1/","title":"Gin 예외처리 - 1. Go Error"},{"content":"도커와 데이터베이스 한참 컨테이너를 공부를 했을 당시 도커의 주 목적이 빠른 배포/개발 환경을 구성하는 것이기 때문에 영속성을 가진 데이터베이스에는 적합하지 않았다고 알고 있다. 이에 대한 문제점으로 DB 데이터 손실을 언급한다. 그런데 도커를 생각해보면 volume 기능이 존재한다. 즉, 데이터를 영속적으로 보관할 수 있다는 것이다. 현구막 기술 블로그에서 보면 이론적으로는 도커를 사용해도 되었으나, 서비스 운영 측면에서는 도커라는 기술이 불안정해 쓰이지 않는다고 언급되어 있다.\n쿠버네티스에서는요? 그런데 저건 도커에 한정된 이야기인 것 같다. 쿠버네티스의 특징을 잘 살펴보면 상태를 사전에 정의하고, 정의된 상태로 운영된다. 운영 측면에서 편리해 보이고, 안전성이 문제라고 한다면 다중화를 하면 된다. 이정훈님의 medium - 실제로 본 DB on Kubernetes 효과에서 언급했다싶이 일부 성능 향상도 보이기도 했다고 한다.\nTodoPoint에서는 어떻게 구성하지? StatefulSet을 사용하자가 결론이다. StatefulSet은 상태 정보를 구성할때 주로 사용된다. 아래의 블로그처럼 CronJob을 활용해서 백업을 구성하도록 설계해야 겠다는 생각이 들었다. Reference https://nangman14.tistory.com/79 ","date":"2024-03-13T00:00:00Z","permalink":"https://s0okju.github.io/p/todopoint-db-design/","title":"TodoPoint - 쿠버네티스 환경에서 DB는 어떻게 구성하지?"},{"content":"Java 진영에서 자주 사용되는 ORM은 JPA이다. 하지만 Go에서는 대표적인 ORM은 없는듯하다. 그런탓에 이런저런 라이브러리가 많은데, 상황에 맞게 라이브러리를 쓰세요 식의 Go스러움을 많이 볼 수 있다.\n어떤 라이브러리를 사용할 것인가? gorm 과거에는 gorm이 많이 쓰인다고 하던데, 여러 문제점이 존재한다고 한다.1개인적으로 최악의 기능은 설정들을 struct tag로 관리하는 것이라고 생각한다.\nent 요즘 뜨고 있는 라이브러리는 FaceBook에서 자체적으로 개발한 ent이다. GO 코드로 스키마를 작성하면 DB에 모델링을 해주며, 특히 Graph 탐색에 특화되어 있다.\n나의 선택은 나의 선택은 ent이다. 일단 대기업이 개발했다는 점에서 1차적으로 신뢰가 간다. sqlboiler가 화려한 벤치마크 결과를 보여줘서 궁금하긴 하지만 문서화가 잘된 ent를 우선적으로 해보고자 한다.\nORM 꼭 필요한 것인가? 웹개발 자체를 얕게 해오면서 ORM에 익숙해졌다. 그러면서 ORM은 선택이 아닌 필수라고 여기면서 사용했던 것 같다. 하지만 ORM 추천글을 읽으면서 꼭 필요한가에 대해 생각하게 된 계기가 되었던 것 같다.2 개발자 입장에서는 CRUD를 추상화된 방법으로 사용할 수 있으며 구현이 용이하고, 가독성이 있는 코드를 사용할 수 있다. 하지만 ORM에 대한 부정적인 입장의 대부분의 과다하게 사용했을 때를 가정한다. 개인적으로는 어떠한 라이브러리든 과다하게 사용하는 것은 오버헤드가 따른다고 생각한다. 또한 DB 설계 자체가 잘못되어서 일 수 있다. 나와 같은 소규모 프로젝트를 사용하는 경우라면 기존 라이브러리에 있는 모든 기능을 쓸 일이 없다. 그러므로 ORM을 쓰려고 한다. 그리고 개발자가 쓰기 편해야 유지보수하기 편할 것 같다는 생각이 더 많이 든다.\nhttps://umi0410.github.io/blog/golang/how-to-backend-in-go-db/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.reddit.com/r/golang/comments/t3bp79/a_good_orm_for_golang/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-03-09T00:00:00Z","permalink":"https://s0okju.github.io/p/go-db-migration/","title":"Go-DB에 마그이션하기"},{"content":"스프링 부트 스프링 부트의 특징은 아래의 문장으로 정리 가능하다.\n스프링 프레임워크는 IoC/DI를 통해 객체 간의 의존 관계를 설정하고, AOP를 통해 핵심 관점과 부가 관점을 분리해 개발하며, PSA를 통해 추상화된 다양한 서비스들을 일관된 방식으로 사용하도록 한다.\nIoC : 객체의 생성과 관리를 개발자가 하는 것이 아니라 프레임워크가 대신하는 것 DI : 외부에서 객체를 주입받아 사용하는 것 AOP : 프로그래밍을 할 때 핵심 관점과 부가 관점을 나누어서 개발하는 것 PSA : 어느 기술을 사용하던 일관된 방식으로 처리하는 것 IoC/DI를 통해 객체 간의 의존 관계를 설정하고, AOP를 통해 핵심 관점(service)과 부가 관점(Repository)를 분리해 개발하며, PSA(JpaRepository)를 통해 추상화된 다양한 서비스들을 일관된 방식으로 사용하도록 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // Service public interface UserService { void createUser(UserRegisterCommand command); User getUserById(Long userId); UserLoginResponse loginUserByEmail(String email); Boolean logoutById(String userId); } @Service public class UserServiceImp implements UserService{ @Autowired // IoC/DI private UserRepository userRepository; @Autowired private UserMapper userMapper; @Override public void createUser(UserRegisterCommand command) { try { userRepository.save(new UserEntity(command.getEmail(), command.getPassword(), command.getUsername())); }catch (DataAccessException e){ throw new DefaultException(ResponseMessage.DATA_ACCESS_ERROR,\u0026#34;user\u0026#34;); }catch (Exception e){ throw new DefaultException(ResponseMessage.DB_UNEXPECTED_ERROR,\u0026#34;user\u0026#34;); } } } // UserRepository // Crud에서 인터페이스 구현체가 자동으로 만들어진다. // PSA public interface UserRepository extends JpaRepository\u0026lt;UserEntity,Long\u0026gt;,UserCustomRepository{ } 의존성 주입 의존성 주입은 스프링 컨테이너가 생성한 객체(빈)을 주입받는 것을 의미한다. 이때 빈은 스프링 컨테이너가 생성하고 관리하는 객체를 의미한다.\n주입을 받을때는 @Autowired를 사용하고, 빈을 등록하고 싶으면 @Component를 선언하면 된다. 프레임워크에 사용되는 대부분의 핵심 어노테이션은(@Repository, @Service 등) @Component가 포함되어 있다.\n의존성 주입에 대해서는 inung_92님의 velog, [Spring]스프링 기초 - DI을 참고하길 바란다.\n스프링 웹 계층 presentation 계층 HTTP 요청 및 응답 처리 비즈니스 계층 비즈니스 로직 구현 퍼시스턴트 계층 스토리지 관련 로직 추가 위와 같은 구조는 Layered Architecture라고 한다. 이때 레이어 간 응집성을 높이고 의존도를 낮추기 위해서는 계층 간 호출 시 인터페이스를 통해 호출해야 한다. 즉, 상위 계층은 직접적으로 하위 계층을 호출하지 않고 추상적인 인터페이스에 의존한다.\nQ. 응집도란?\n응집도란 한 모듈 내부의 처리 요소들이 서로 관련되어 있는 정도를 말한다. 즉, 모듈이 독립적인 기능을 수행하지 또는 하나의 기능을 중심으로 책임이 잘 뭉쳐있는지를 나타내며 모듈이 높은 응집도를 가질수록 좋다.\n출처 - https://madplay.github.io/post/coupling-and-cohesion-in-software-engineering\nQ. 의존성을 왜 낮춰야 하는 걸까?\nA. 의존성이란 다른 객체의 영향을 받고, 다른 객체에 따라 결과가 좌우되는 것이다. 의존성을 낮춰 객체들 간 전파되는 변경에 대한 영향을 최소화 시켜, 유지보수 시 원활하게 하기 위해서이다.\n출처 - https://velog.io/@ung6860/Spring스프링-기초-DIDependency-Injection\nQ. 의존성을 낮추는 방법?\nA. 기본적으로 의존성을 클래스 내부에서 new 연산자를 사용하는 것이 아닌 외부로부터 객체의 의존성을 주입받아야한다. 외부로부터 의존성을 주입받는 것의 의미는 \u0026lsquo;의존성 주입\u0026rsquo;에 초점을 맞추는게 아니라 \u0026lsquo;의존성 탈피를 위하여 외부로부터 주입\u0026rsquo;이라는 부가적인 설명에 초점을 맞추어야한다.\nPresentation 웹 클라이언트의 요청 및 응답 처리한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @RestController @RequestMapping(\u0026#34;/membership/\u0026#34;); public class RegisterMembershipController { @Autowired private MembershipService membershipService; @PostMapping(path = \u0026#34;register\u0026#34;) public Membership createmembership(@RequestBody MembershipRequest request){ MembershipCommand command = MembershipCommand.builder() .name(\u0026#34;hello\u0026#34;).builder(); return membershipService.createMembership(command); } 보통은 @Controller를 통해 controller를 지정해줄 수 있다. rest api를 구현했는데, 이에 적합한 @RestController가 있다. @RestController는 @Controller + @ResponseBody라고 보면 된다. 자세한 사항은 망나니 개발자님의 블로그를 참고하길 바란다.\nService 비즈니스 로직을 처리하는 계층이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public interface MembershipService{ Membership createMembership(MembershipCommand command); } @Service public class MembershipServiceImp implements MembershipService{ @Autowired MembershipRepository membershipRepository; @Override Membership createMembership(MembershipCommand command){ membershipRepository.save(command); } } 핵심 비즈니스 로직은 연산으로 구성된 인터페이스를 정의하고 그에 대한 구현을 따로 작성하는 것이 좋다. 그래서 컨트롤러가 인터페이스에 의존하게 되는데, 나중에서 서비스 구현이 변경되더라도 인터페이스에 의존돼 컨트롤러를 변경할 필요가 없다는 것이다.\nRepository 저장소에 접근하는 영역으로 DAO(Data Access Object) 영역이라고 부른다. @Repository를 사용하여 Spring repository를 지정해준다. 주목할 점은 인터페이스 내용이 비어 있더라도 스프링 데이터가 JPA 런타임에 인터페이스 구현체를 자동으로 만들어준다.\n1 2 3 4 @Repository public interface MembershipRepository extends CrudRepository\u0026lt;Membership,Long\u0026gt; { } DTO 계층 간에 데이터 교환을 위한 객체를 의미한다.\n처음 공부 했을 때 “Dto 다 똑같은 객체겠군… 오류 발생하면 난리나겠네”라고 생각했다. 각 계층마다 Dto가 다를 수 있다. 아래의 예시를 보면 Controller → Service에서는 Dto가 Membership 객체를 사용했다. 하지만 Membership 도메인과 실제 엔티티와 다를 경우라면 어떨까? Service → Repository에서는 Dto가 MembershipEntity가 되는 것이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // Controller membershipService.createMembership(member); // MembershipService public interface MembershipService{ Membership createMembership(Membership member); } @Component public class MembershipMapper { MembershipEntity MapToEntity(Membership member){ return new MembershipEntity( // ... 생략 ); } } @Service public class MembershipServiceImp implements MembershipService{ @Autowired MembershipRepository MembershipRepository; @Override Membership createMembership(Membership membership){ MembershipRepository.save(MembershipMapper.MapToEntity(membership)); } } 테스트 소프트웨어 개발 시 테스트 단계는 필수다. 이 단계에서는 작성한 코드를 기반으로 검증한다. 테스트는 크게 단위 테스트와 통합 테스트가 존재한데, 단위 테스트를 집중적으로 설명하고자 한다.\n단위 테스트란 하나의 모듈을 기준으로 독립적으로 진행되는 가장 작은 단위의 테스트이다. 특정 부분만 독립적으로 테스트하기 때문에 빠르게 문제 여부를 확인할 수 있게 된다.\n자세한 사항은 망나니 개발자님의 블로그를 참고하길 바란다.\n데이터베이스 JPA(Java Persistent API) 관계형 데이터베이스를 사용하는 방식을 정의한 인터페이스이다. 이를 구현한 ORM 프레임워크로는 hibernate가 있다.\nJPA는 다양한 쿼리 방법을 지원한다.\nJPQL QueryDSL Native SQL JDBC API 직접 사용 요즘은 QueryDSL를 사용하는 추세라고 한다.\n앤티티 DB의 테이블과 매핑되는 객체로 DB 테이블과 직접 연결된다.\n영속성 컨텍스트 엔티티를 관리하는 가상 공간이다.\n1차 캐시 캐시키는 엔티티의 키로 둬서, 조회시 1차적으로 영속성 컨텍스트 내에 있는 값을 반환한다. 쓰기 지연 빈번한 context swtiching에 대한 overhead를 줄여주기 위해 한꺼번에 모았다가 처리한다. 변경 감지 영속성 컨텍스트 내에 있는 캐시와 변경된 entity값을 비교해서 엔티티 변경 감지 후 DB에 자동 반영 비즈니스 로직, DB 상에 있는 로직의 꼬이는 것을 방지 Q. 왜 영속성 테스트를 사용하는 것인가?\nA. 트랜잭션의 횟수나 방식을 효율적으로 관리해 서버의 속도를 늘리고, 중복되는 쿼리를 줄일 수 있다.\nWAS가 시작되면 EntityMangerFactory가 생성되고, 트랜잭션마다 생성되는 EntityManager를 관리한다. 이때 EntityManager는 Entity의 생명 주기를 관리한다.(관리 상태, 비영속 상태, 삭제된 상태)\n스프링 부트는 EntityManagerFactory를 하나만 생성해서 관리하고, 의존성 주입해서 사용함. 빈을 하나만 생성해서 동시성 문제를 해소했다.\nSpring data api 인터페이스 및 주석 세트를 제공해서 데이터 액세스 계층을 쉽게 구축할 수 있도록 한다. JPA를 한 단계 추상화시킨 Repository라는 인터페이스를 제공함으로써 이뤄진다.\n스프링 데이터의 공통적인 기능에서 JPA의 유용한 기술이 추가 (스프링 데이터 인터페이스를 상속받아서 간단한 트랜잭션 처리 가능)\nReference Spring - ORM, JPA, Hibernate, JDBC 총정리 [JPA] 객체지향 쿼리, JPQL [TDD] 단위 테스트(Unit Test) 작성의 필요성 (1/3) [Spring]스프링 기초 - DI(Dependency Injection) ","date":"2024-02-03T00:00:00Z","permalink":"https://s0okju.github.io/p/basic-spring-1/","title":"Spring - Spring Boot 간략하게 알아보기"},{"content":"Terraform 이란 HashiCorp에서 오픈소스로 개발중인, 클라우드 및 온프로미스 인프라를 코드로 관리할 수 있는 코드이다. 인프라 환경 구성 시 선언적 코드형식을 사용하여 리소스를 생성, 수정, 삭제하여 관리가 가능한 laC(Infrastructure as Code) 프로비저닝 도구이다.\nTerraform 작동 방식 주로 Write, Plan, Apply 절차로 이뤄진다.\nWrite : Hashicorp에서 자체 개발한 HCL 언어로 스크립트 작성 Plan : 상태를 비교하며 변경점을 사용자에게 보여줌 Apply : 실행하는 단계, 순차적으로 실행 의존 관계에 따라 순서를 명시할 수 있다. 상태 비교를 통해 휴먼 에러를 줄일 수 있다. 그러나 코드의 무게가 무거워지면, 실행하는데 오래걸리는 원인이 될 수 있다. 그 외에도 다양한 단점이 존재할 것이다. 그럼에도 불구하고 생산성 측면, 종속성 그래프 등 장점들이 더 크기 때문에 사용하는 것이다.\n클라우드 컴퓨팅 환경은 참조된 리소스가 처음의 리소스를 참조하는 순환루프 문제를 지니고 있다. 무한한 반복으로 인해 시스템이 빠르게 고갈되고, 성능이 저하된다. Terraform에서는 리소스의 참조 내용을 그래프로 보여주는 Resource Graph를 제공해주는데, 이를 활용해 순환루프 문제를 예방할 수 있다.\n연습 자세한 문법은 Terraform Language Documentation을 참고하길 바란다.\n시나리오 AWS를 활용한다. 개발, 운영 VPC를 구축한다. 각 VPC에서 private, public subnet이 존재한다. 외부와 통신을 위해 internet gateway와 내부 리소스끼리만 통신할 수 있는 NAT gateway를 설치한다. 구현 AWS를 활용한다. Terraform AWS Registry에서 제공해주는 코드를 그대로 사용한다. 나의 경우 region을 ap-northeast-2 로 설정했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } # Configure the AWS Provider provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } 개발 운영 vpc를 구축한다. vpc 모듈을 별도로 만들어 코드를 재사용했고, env를 변수로 삼아 리소스의 이름에 개발 환경을 명시하도록 했다.\n1 2 3 4 5 6 7 8 9 10 11 // For Dev module \u0026#34;dev_vpc\u0026#34; { source = \u0026#34;./vpc.d\u0026#34; env = \u0026#34;dev\u0026#34; } // For Production module \u0026#34;prd_vpc\u0026#34; { source = \u0026#34;./vpc.d\u0026#34; env = \u0026#34;prd\u0026#34; } 모듈의 main.tf에는 public, private Subnet 그리고 NAT gateway, Internet gateway를 생성하는 코드를 작성했다.\nPrivate Subnet을 구성하기 위해서는 라우팅 테이블이 필요하지만, 임시이므로 Private NAT Gateway만 작성했다. 여기서 주목해야 할 점은 NAT, Internet Gateway의 경우 Subnet이 우선적으로 만들어져야 실행된다.(만약에 그러지 않는다면 오류가 뜬다.) 그러므로 코드를 작성할때 각 리소스의 의존성을 고려해서 작성해야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } // Configure the AWS Provider provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } // Create a VPC resource \u0026#34;aws_vpc\u0026#34; \u0026#34;default\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; tags = { Name = \u0026#34;terraform_default_vpc_${var.env}\u0026#34; } } // public subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnet_1\u0026#34; { vpc_id = aws_vpc.default.id cidr_block = \u0026#34;10.0.0.0/24\u0026#34; tags = { Name = \u0026#34;terraform_public_subnet_1_${var.env}\u0026#34; } } // private subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnet_1\u0026#34; { vpc_id = aws_vpc.default.id cidr_block = \u0026#34;10.0.10.0/24\u0026#34; tags = { Name = \u0026#34;terraform_private_subnet_1_${var.env}\u0026#34; } } // private NAT resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;private_nat\u0026#34; { connectivity_type = \u0026#34;private\u0026#34; subnet_id = aws_subnet.private_subnet_1.id tags = { Name = \u0026#34;terraform_nat_${var.env}\u0026#34; } } // Internet gateway resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;igw\u0026#34; { vpc_id = aws_vpc.default.id tags = { Name = \u0026#34;terraform_igw_${var.env}\u0026#34; } } Terraform 도입과 관련된 기술 블로그 좌충우돌 Terraform 입문기, 우아한형제들 기술블로그 DevOps팀의 Terraform 모험 Terraform IaC 도구를 활용한 AWS 웹콘솔 클릭 노가다 해방기 → 다들 웹콘솔에서의 해방을 외치고 있었다..\n","date":"2024-01-25T00:00:00Z","permalink":"https://s0okju.github.io/p/basic-terraform-1/","title":"Terraform 기초 맛보기 - 1. 기본 문법 익히기"}]